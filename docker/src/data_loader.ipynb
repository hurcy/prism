{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "from datetime import datetime, timedelta, time as datetime_time, timezone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "VALUE_MAP = ['HR','RR','SpO2','Pulse','Temp','ABPm','ABPd','ABPs','NBPm','NBPs','NBPd','SPO2-%','SPO2-R',\n",
    "'Resp','PVC','ST-II','etCO2','SpO2 r','imCO2','ST-V1','ST-I','ST-III','ST-aVF','ST-aVL','ST-aVR',\n",
    "'awRR','CVPm','AoM','ST-V2','ST-V3','ST-V4','ST-V5','ST-V6','SpO2T','T1','TV','Cdyn','PEEP','RRaw',\n",
    "'TVin','inO2','AoD','AoS','InsTi','MINVOL','MnAwP','PIP','MVin','PB','Poccl','Pplat',\n",
    "'MV','Patm','Ppeak','Rinsp','ST-V','sInsTi','sPEEP','sTV','sTrig','sPSV','Rexp','highP',\n",
    "'sAPkFl','sAWRR','sFIO2','sPIF','sMV','sO2','sRisTi','ARTd','ARTm','ARTs','PAPm','sSIMV']\n",
    "\n",
    "MEASUREMENT_NORMALIZATION = ['mean', 'predefined']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='../../data/train'\n",
    "common_path='../../data/volume'\n",
    "\n",
    "task_path='../../data/volume/local_test'\n",
    "# os.mkdir(task_path)\n",
    "is_train=True\n",
    "\n",
    "group_hour=1\n",
    "timestep_per_data=128\n",
    "\n",
    "measurement_normalize='mean'\n",
    "\n",
    "condition_min_limit=0\n",
    "condition_group=False\n",
    "\n",
    "valid_size=0.2\n",
    "data_split_random_seed=1235\n",
    "pytest=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.018996000289916992\n",
      "data_loader extract_person time: 0.010002374649047852\n",
      "data_loader extract_condition time: 0.009009838104248047\n",
      "data_loader extract_measurement time: 2.6911568641662598\n"
     ]
    }
   ],
   "source": [
    "  def extract_outcome_cohort():\n",
    "    start_time = time.time()\n",
    "    cohort_df = pd.read_csv(os.path.join(data_path, 'OUTCOME_COHORT.csv'), encoding='windows-1252')\n",
    "\n",
    "    cohort_df.COHORT_START_DATE = pd.to_datetime(cohort_df.COHORT_START_DATE)\n",
    "    cohort_df.COHORT_END_DATE = pd.to_datetime(cohort_df.COHORT_END_DATE)\n",
    "    print(\"data_loader extract_outcome_cohort time:\", time.time() - start_time)\n",
    "    return cohort_df\n",
    "\n",
    "  def extract_person():\n",
    "    start_time = time.time()\n",
    "    person_df = pd.read_csv(os.path.join(data_path, 'PERSON_NICU.csv'), encoding='windows-1252')\n",
    "    person_df = pd.concat([\n",
    "        person_df[['PERSON_ID', 'BIRTH_DATETIME']],\n",
    "        pd.get_dummies(person_df.GENDER_SOURCE_VALUE, prefix='gender')\n",
    "    ], axis=1)\n",
    "\n",
    "    # 생일 컬럼 타입 설정\n",
    "    person_df.BIRTH_DATETIME = pd.to_datetime(person_df.BIRTH_DATETIME, utc=True)\n",
    "    # 여성/남성 컬럼 1개로 처리\n",
    "    person_df.rename(columns={'gender_M': 'GENDER'}, inplace=True)\n",
    "    if 'gender_F' in person_df.columns:\n",
    "      del person_df['gender_F']\n",
    "\n",
    "    print(\"data_loader extract_person time:\", time.time() - start_time)\n",
    "    return person_df\n",
    "\n",
    "  def extract_condition():\n",
    "    start_time = time.time()\n",
    "    condition_df = pd.read_csv(os.path.join(data_path, 'CONDITION_OCCURRENCE_NICU.csv'), encoding='windows-1252',\n",
    "                               usecols=['PERSON_ID', 'CONDITION_SOURCE_VALUE', 'CONDITION_START_DATETIME'])\n",
    "    # Null 이거나 값이 빈 것을 날림\n",
    "    condition_df = condition_df[pd.notnull(condition_df.CONDITION_SOURCE_VALUE)]\n",
    "    condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.str.len() > 0]\n",
    "\n",
    "    if condition_group:\n",
    "      condition_df.CONDITION_SOURCE_VALUE = condition_df.CONDITION_SOURCE_VALUE.str.slice(stop=3)\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    condition_df.CONDITION_START_DATETIME = pd.to_datetime(condition_df.CONDITION_START_DATETIME, utc=True)\n",
    "\n",
    "    print(\"data_loader extract_condition time:\", time.time() - start_time)\n",
    "    return condition_df\n",
    "\n",
    "  def extract_measurement():\n",
    "    start_time = time.time()\n",
    "    measurement_df = pd.read_csv(os.path.join(data_path, 'MEASUREMENT_NICU.csv'), \n",
    "                                 encoding='windows-1252',\n",
    "                                 usecols=['PERSON_ID', 'MEASUREMENT_DATETIME',\n",
    "                                          'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "                                 )\n",
    "#     if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "#       # source_value 맵핑\n",
    "#       source_value_invert_map = {}\n",
    "#       for new_value in MEASUREMENT_SOURCE_VALUE_MAP:\n",
    "#         for table_value in MEASUREMENT_SOURCE_VALUE_MAP[new_value]:\n",
    "#           source_value_invert_map[table_value] = new_value\n",
    "#       measurement_df.MEASUREMENT_SOURCE_VALUE = measurement_df.MEASUREMENT_SOURCE_VALUE.replace(source_value_invert_map)\n",
    "\n",
    "      # 맵핑이된 정보만 남긴다\n",
    "    measurement_df = measurement_df[measurement_df.MEASUREMENT_SOURCE_VALUE.isin(VALUE_MAP)]\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    measurement_df.MEASUREMENT_DATETIME = pd.to_datetime(measurement_df.MEASUREMENT_DATETIME, utc=True)\n",
    "\n",
    "    # source_value별 평균값 추출\n",
    "    if is_train:\n",
    "      measurement_mean_df = measurement_df.groupby('MEASUREMENT_SOURCE_VALUE').VALUE_AS_NUMBER.mean()\n",
    "      measurement_mean_df.to_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "    else:\n",
    "      # inference일 경우 저장된 걸 불러온다\n",
    "      measurement_mean_df = pd.read_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "\n",
    "    print(\"data_loader extract_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df, measurement_mean_df\n",
    "cohort_df = extract_outcome_cohort()\n",
    "person_df = extract_person()\n",
    "condition_df = extract_condition()\n",
    "measurement_df,measurement_mean_df = extract_measurement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def groupby_hour_condition( condition_df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    condition_df['CONDITION_DATE'] = condition_df.CONDITION_START_DATETIME.dt.date\n",
    "    condition_df['CONDITION_DATE'] = pd.to_datetime(condition_df.CONDITION_DATE, utc=True)\n",
    "\n",
    "    if is_train and condition_min_limit > 0:\n",
    "      condition_group = condition_df.groupby('CONDITION_SOURCE_VALUE').PERSON_ID.count()\n",
    "      condition_group = condition_group[condition_group > condition_min_limit].index\n",
    "\n",
    "      condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.isin(condition_group)]\n",
    "\n",
    "    # 진단은 시간이 없다. 당일의 마지막에 진단 받은걸로 가정한다\n",
    "    condition_df['HOURGRP'] = 23 // group_hour\n",
    "\n",
    "    group_cols = ['PERSON_ID', 'CONDITION_DATE', 'HOURGRP', 'CONDITION_SOURCE_VALUE']\n",
    "\n",
    "    condition_df['DUMMY'] = condition_df['CONDITION_SOURCE_VALUE']\n",
    "    condition_df = condition_df.groupby(group_cols) \\\n",
    "        .DUMMY.count().unstack().reset_index().fillna(0)\n",
    "\n",
    "    condition_df = condition_df.rename(columns={'CONDITION_DATE': 'DATE'})\n",
    "\n",
    "    condition_col_filename = os.path.join(task_path, 'condition_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(condition_col_filename, np.array(condition_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      condition_cols = np.load(condition_col_filename, allow_pickle=True)\n",
    "      new_condition_list = []\n",
    "      for col in condition_cols:\n",
    "        if col in condition_df.columns:\n",
    "          new_condition_list.append(condition_df[col])\n",
    "        else:\n",
    "          new_condition_list.append(pd.Series([0] * condition_df.shape[0]))\n",
    "\n",
    "      condition_df = pd.concat(new_condition_list, axis=1)\n",
    "      condition_df.columns = condition_cols\n",
    "    print(\"data_loader groupby_hour_condition time:\", time.time() - start_time)\n",
    "    return condition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader groupby_hour_condition time: 0.018995285034179688\n"
     ]
    }
   ],
   "source": [
    "condition_df = groupby_hour_condition(condition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _clip_measurement(measurement_source_value, value_as_number):\n",
    "    if value_as_number > MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']\n",
    "    elif value_as_number < MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']\n",
    "    return value_as_number\n",
    "\n",
    "  def groupby_hour_measurement(measurement_df):\n",
    "    start_time = time.time()\n",
    "    # timestamp로 join 하기 위하여 시간 포맷을 utc로 통일\n",
    "    measurement_df['MEASUREMENT_DATE'] = measurement_df.MEASUREMENT_DATETIME.dt.date\n",
    "    measurement_df['MEASUREMENT_DATE'] = pd.to_datetime(measurement_df.MEASUREMENT_DATE, utc=True)\n",
    "\n",
    "    measurement_df['MEASUREMENT_HOUR'] = measurement_df.MEASUREMENT_DATETIME.dt.hour\n",
    "    measurement_df['MEASUREMENT_HOURGRP'] = measurement_df.MEASUREMENT_HOUR // group_hour\n",
    "\n",
    "    # 평균값 이용하여 Normalize\n",
    "    if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "      measurement_df = pd.merge(measurement_df,\n",
    "                                measurement_mean_df.reset_index().rename(\n",
    "                                    columns={'VALUE_AS_NUMBER': 'MEAN_VALUE'}),\n",
    "                                on='MEASUREMENT_SOURCE_VALUE', how='left')\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.VALUE_AS_NUMBER / measurement_df.MEAN_VALUE\n",
    "    # 생체신호 범위를 이용하여 Normalize\n",
    "    elif measurement_normalize == MEASUREMENT_NORMALIZATION[1]:\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.apply(lambda row:\n",
    "                                                            _clip_measurement(\n",
    "                                                                row['MEASUREMENT_SOURCE_VALUE'],\n",
    "                                                                row['VALUE_AS_NUMBER']),\n",
    "                                                            axis=1)\n",
    "\n",
    "      # TODO\n",
    "    group_cols = ['PERSON_ID', 'MEASUREMENT_DATE', 'MEASUREMENT_HOURGRP', 'MEASUREMENT_SOURCE_VALUE']\n",
    "    agg_list = ['count', 'min', 'max', 'mean', 'std', 'var']\n",
    "    measurement_df['VALUE_DIFF'] = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.diff()\n",
    "\n",
    "    measurement_diff_df = pd.pivot_table(measurement_df, \n",
    "                                         values='VALUE_DIFF', index=group_cols[:-1],\n",
    "                                         columns='MEASUREMENT_SOURCE_VALUE', aggfunc=['mean','max','min'])\n",
    "\n",
    "    measurement_diff_df.columns = [('diff', '{}_{}'.format(v[0],v[1])) for v in measurement_diff_df.columns]\n",
    "\n",
    "    measurement_df = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.agg(agg_list).fillna(0).unstack().fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    measurement_df = pd.concat([measurement_df, measurement_diff_df], axis=1).reset_index()\n",
    "\n",
    "    if measurement_df.isnull().sum().sum() >0:\n",
    "        print(\"there is Na after interpolation\")\n",
    "        measurement_df = measurement_df.fillna(0)\n",
    "        \n",
    "    # 사용한 후 삭제\n",
    "    del measurement_diff_df\n",
    "    # 컬럼 이름 정제 (그룹화 하기 쉽게)\n",
    "    new_cols = []\n",
    "    for col in measurement_df.columns:\n",
    "      \n",
    "      if col[1] == '':\n",
    "        new_cols.append(col[0])\n",
    "      elif col[0] in agg_list + ['diff']:\n",
    "        new_cols.append((col[1], col[0]))\n",
    "    measurement_df.columns = new_cols\n",
    "\n",
    "#     #minmax scale\n",
    "#     scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "#     scaler = scaler.fit(measurement_df.iloc[:,3:])\n",
    "#     measurement_df.iloc[:,3:] = scaler.transform(measurement_df.iloc[:,3:])\n",
    "    \n",
    "    measurement_df = measurement_df.rename(columns={'MEASUREMENT_DATE': 'DATE',\n",
    "                                                    'MEASUREMENT_HOURGRP': 'HOURGRP'})\n",
    "\n",
    "    measurement_col_filename = os.path.join(task_path, 'measurement_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(measurement_col_filename, np.array(measurement_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      measurement_cols = np.load(measurement_col_filename, allow_pickle=True)\n",
    "      new_measurement_list = []\n",
    "      for col in measurement_cols:\n",
    "        if col in measurement_df.columns:\n",
    "          new_measurement_list.append(measurement_df[col])\n",
    "        else:\n",
    "          new_measurement_list.append(pd.Series([0] * measurement_df.shape[0]))\n",
    "\n",
    "      measurement_df = pd.concat(new_measurement_list, axis=1)\n",
    "      measurement_df.columns = measurement_cols\n",
    "    print(\"data_loader groupby_hour_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 7.178995132446289\n"
     ]
    }
   ],
   "source": [
    "measurement_df = groupby_hour_measurement(measurement_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder 작업중 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.005460262298583984\n",
      "data_loader extract_person time: 0.003716707229614258\n",
      "data_loader extract_condition time: 0.0031468868255615234\n",
      "data_loader extract_measurement time: 2.900022029876709\n",
      "data_loader groupby_hour_condition time: 0.0072422027587890625\n",
      "data_loader groupby_hour_measurement time: 8.61219596862793\n",
      "data_loader make_person_sequence time: 0.08829355239868164\n",
      "X (2592,)\n",
      "Y (2592,)\n",
      "Key (2592, 2)\n",
      "data_loader make_data time: 0.19215607643127441\n",
      "data_loader split_data time: 0.272402286529541\n"
     ]
    }
   ],
   "source": [
    "from data_loader import DataLoader\n",
    "import os\n",
    "task_path='../../data/volume/local_test'\n",
    "data_loader = DataLoader(data_path=os.path.join('../../data', 'train'),\n",
    "                         common_path=os.path.join(\"../../data\", 'volume'),\n",
    "                         measurement_normalize='mean',\n",
    "                         is_train = True,\n",
    "                         task_path=task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.layers import GRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, Masking, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import TruncatedNormal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = data_loader.train_x.shape\n",
    "shaped_x = data_loader.train_x.reshape(-1,shape[2])\n",
    "\n",
    "shape_v = data_loader.valid_x.shape\n",
    "shaped_v = data_loader.valid_x.reshape(-1,shape_v[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(254976, 84)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(data_loader.train_x.shape[2],))\n",
    "x = Dense(32,activation = 'tanh')(input_img)\n",
    "\n",
    "output = Dense(data_loader.train_x.shape[2])(x)\n",
    "\n",
    "# 입력을 입력의 재구성으로 매핑할 모델\n",
    "autoencoder = Model(input_img, output)\n",
    "\n",
    "\n",
    "autoencoder.compile(loss='mean_squared_error',optimizer=adam(lr = 0.003))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 254976 samples, validate on 76800 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.0015 - val_loss: 0.0467\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0014 - val_loss: 0.0445\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0014 - val_loss: 0.0456\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0014 - val_loss: 0.0462\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0014 - val_loss: 0.0445\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0013 - val_loss: 0.0445\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0013 - val_loss: 0.0454\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0013 - val_loss: 0.0448\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0013 - val_loss: 0.0442\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0013 - val_loss: 0.0446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b8ae5d4b88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(shaped_x, shaped_x, batch_size=len(shaped_x)//3, \n",
    "                                    epochs=10, verbose=2, \n",
    "                                    validation_data=(shaped_v, shaped_v)\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = autoencoder.predict(shaped_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89136064, -0.99954075, -1.0432813 , -0.99191695, -1.0129839 ,\n",
       "       -1.0053903 , -1.0031613 , -0.98737705, -0.36665756, -0.6894814 ,\n",
       "       -0.9987129 , -0.06164925, -0.8731014 , -0.98848146, -1.0398297 ,\n",
       "       -0.96429247, -0.9990317 , -0.9790456 , -0.41440856,  0.7082262 ,\n",
       "       -0.9545926 ,  0.8991566 , -0.8440903 , -0.9732426 , -1.0390455 ,\n",
       "       -0.99509436, -1.0007738 , -0.96198595,  0.1580378 ,  0.39751974,\n",
       "       -0.9567536 ,  0.94732785, -0.8593732 , -0.97982854, -1.0473773 ,\n",
       "       -0.96758676, -1.0054554 , -0.97184557,  0.4063867 ,  0.67086035,\n",
       "       -0.9504767 ,  0.90166724, -0.9295129 , -0.9762178 , -0.96051204,\n",
       "       -1.0299596 , -0.9585086 , -0.9665948 ,  0.27067655, -0.83907646,\n",
       "       -1.024588  , -0.97660893, -0.9473959 , -0.9780998 , -1.0177491 ,\n",
       "       -1.0327438 , -1.0280795 , -0.991439  , -0.12029615, -1.0520234 ,\n",
       "       -1.0096073 , -1.0558196 ,  0.08194655,  0.05355515,  0.5069759 ,\n",
       "       -0.2819863 ,  0.479878  ,  0.15720716, -0.08621441,  0.84526134,\n",
       "       -0.11089681,  0.70415694, -0.99082434, -0.9522029 , -0.97174513,\n",
       "       -0.9685004 , -0.9680137 , -0.970328  , -0.95674163, -1.0028956 ,\n",
       "       -1.0024705 , -0.97219265, -0.98247564, -0.9674065 ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.4083333 , -0.75      ,\n",
       "       -1.        , -0.0666666 , -1.        , -1.        , -1.        ,\n",
       "       -0.9809524 , -1.        , -1.        , -0.46448088,  0.8490566 ,\n",
       "       -1.        ,  0.8383838 , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        ,  0.07590759,  0.32631588,\n",
       "       -1.        ,  0.96000004, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        ,  0.41959465,  0.7027502 ,\n",
       "       -1.        ,  0.88334966, -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        ,  0.29791903, -0.7555192 ,\n",
       "       -1.        , -0.8702868 , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -0.15770304, -0.9701146 ,\n",
       "       -1.        , -0.9915872 ,  0.07103837,  0.0333333 ,  0.45098042,\n",
       "       -0.2800814 ,  0.5789474 ,  0.15254235, -0.14025003,  0.85078216,\n",
       "       -0.10344833,  0.7145566 , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        , -1.        ,\n",
       "       -1.        , -1.        , -1.        , -1.        ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shaped_x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(self.data_loader.train_x.shape[1], self.data_loader.train_x.shape[2],))\n",
    "layer1 = self.model.layers[2]\n",
    "layer1.trainable = False\n",
    "\n",
    "x = layer1(input_img)\n",
    "x = GRU(32, activation = 'relu') (x)\n",
    "x = Dense(16, activation = 'relu') (x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output = Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "model = Model(input_img, output)\n",
    "model.compile(optimizer=adam(lr = 0.003), loss=loss, metrics=['accuracy'])\n",
    "\n",
    "self.rnnmodel = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-deb42b631813>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mdocker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m train_measure, valid_measure = train_test_split(measurement_df,\n\u001b[0;32m      5\u001b[0m                                                       \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'docker'"
     ]
    }
   ],
   "source": [
    "from docker.src.model import Autoencoder\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "\n",
    "callbacks = [\n",
    "ModelCheckpoint(filepath=os.path.join(task_path, 'encoder-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=True\n",
    "),\n",
    "EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-778e9d08c7a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import model_from_json \n",
    "\n",
    "\n",
    "model_path = tf.train.latest_checkpoint(task_path)\n",
    "if model_path is None:\n",
    "  file_name = sorted([file_name for file_name in os.listdir(task_path) if file_name.endswith('.hdf5') and file_name.startswith('encoder')])[-1]\n",
    "  model_path = os.path.join(task_path, file_name)\n",
    "\n",
    "model = model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder-10-725.665955.hdf5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "input_img = Input(shape=(train_measure.iloc[:,3:].shape[1],))\n",
    "layer1=autoen.model.layers[1]\n",
    "layer2=autoen.model.layers[2]\n",
    "\n",
    "encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "output=encoder.predict(train_measure.iloc[:,3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros([100,20,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 20, 4)             0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 20, 32)            3552      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20, 4)             132       \n",
      "=================================================================\n",
      "Total params: 3,684\n",
      "Trainable params: 3,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU\n",
    "from keras.optimizers import adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, Masking, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "input_img = Input(shape=(a.shape[1],a.shape[2],))\n",
    "x = GRU(32,activation = 'tanh',\n",
    "          return_sequences=True)(input_img)\n",
    "\n",
    "output = Dense(a.shape[2], activation ='tanh')(x)\n",
    "\n",
    "# 입력을 입력의 재구성으로 매핑할 모델\n",
    "autoencoder = Model(input_img, output)\n",
    "\n",
    "\n",
    "autoencoder.compile(loss='mean_squared_error',optimizer=adam(lr = 0.003))\n",
    "\n",
    "print(autoencoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-caedb7e2d1a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_measure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlayer1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "    input_img = Input(shape=(self.train_measure.shape[1],))\n",
    "    layer1=self.model.layers[1]\n",
    "    layer2=self.model.layers[2]\n",
    "\n",
    "    encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "    output=encoder.predict(total_measure)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                          train_size=(1 - valid_size),\n",
    "                                                          test_size=valid_size,\n",
    "                                                          random_state=data_split_random_seed)\n",
    "    autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "    autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "    self.model = autoen\n",
    "    output = autoen.predict(measurement_df.iloc[:,3:])\n",
    "    mesurement_df[:,3:] = output\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "\n",
    "output = autoen.predict(measurement_df.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.3929615020751953\n",
      "data_loader extract_person time: 0.025004863739013672\n",
      "data_loader extract_condition time: 0.031996965408325195\n",
      "data_loader extract_measurement time: 2.231036424636841\n",
      "data_loader groupby_hour_condition time: 0.010988235473632812\n",
      "condition_shape :  (55, 15)\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 6.492940425872803\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               13932     \n",
      "=================================================================\n",
      "Total params: 44,396\n",
      "Trainable params: 44,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.2831 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08259, saving model to ./data/volume/local_test\\encoder-01-0.08.hdf5\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0747 - val_loss: 0.0737\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08259 to 0.07367, saving model to ./data/volume/local_test\\encoder-02-0.07.hdf5\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0647 - val_loss: 0.0640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07367 to 0.06402, saving model to ./data/volume/local_test\\encoder-03-0.06.hdf5\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0557 - val_loss: 0.0484\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06402 to 0.04836, saving model to ./data/volume/local_test\\encoder-04-0.05.hdf5\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0447 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04836 to 0.04168, saving model to ./data/volume/local_test\\encoder-05-0.04.hdf5\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0386 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04168 to 0.03562, saving model to ./data/volume/local_test\\encoder-06-0.04.hdf5\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0320 - val_loss: 0.0315\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03562 to 0.03149, saving model to ./data/volume/local_test\\encoder-07-0.03.hdf5\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0288 - val_loss: 0.0281\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03149 to 0.02812, saving model to ./data/volume/local_test\\encoder-08-0.03.hdf5\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0261 - val_loss: 0.0255\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02812 to 0.02551, saving model to ./data/volume/local_test\\encoder-09-0.03.hdf5\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0232 - val_loss: 0.0233\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02551 to 0.02329, saving model to ./data/volume/local_test\\encoder-10-0.02.hdf5\n",
      "data_loader autoencoder time: 5.81405234336853\n",
      "data_loader make_person_sequence time: 0.07797789573669434\n",
      "X (1592,)\n",
      "Y (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.201002836227417\n",
      "on_split\n",
      "data_loader split_data time: 0.04504966735839844\n"
     ]
    }
   ],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "data_path = data_path='./data'\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         is_train = True,\n",
    "                         task_path=task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from docker.src.data_loader import DataLoader\n",
    "from docker.src.model import SimpleRNNModel\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import datetime\n",
    "\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "\n",
    "task_log_path = os.path.join(log_path, 'local_test')\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_loader, fraction, repeat):\n",
    "        'Initialization'\n",
    "        self.xt, self.yt,self.nx, self.ny = data_loader()\n",
    "        self.fraction = fraction\n",
    "        self.repeat = repeat\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.nx) / 5))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "            #positive_valid patient중 negative data undersampling    \n",
    "        rand_false2 = np.random.choice(self.nx.shape[0], size=int(np.floor(self.nx.shape[0]*self.fraction)))\n",
    "        random_nx = self.nx[rand_false2]\n",
    "        random_ny = self.ny[rand_false2]\n",
    "        \n",
    "        xt = np.repeat(self.xt, self.repeat, axis=0)\n",
    "        yt = np.repeat(self.yt, self.repeat, axis=0)\n",
    "        \n",
    "        train_x = np.concatenate([xt,random_nx], axis=0)\n",
    "        train_y = np.concatenate([yt,random_ny], axis=0)\n",
    "            \n",
    "        if len(train_x) == len(train_y):\n",
    "            p = np.random.permutation(len(train_x))\n",
    "            train_x = train_x[p]\n",
    "            self.train_y = train_y[p]  \n",
    "        else:\n",
    "            print(\"there is non match\")\n",
    "        self.train_x = pad_sequences(train_x, padding='post', value=-5)\n",
    "        return (self.train_x, self.train_y)   \n",
    "    \n",
    "    def shape(self):\n",
    "        return self.train_x.shape\n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'model-{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    TensorBoard(log_dir=task_log_path,\n",
    "                write_graph=True\n",
    "    ),\n",
    "     EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=2, mode='auto')\n",
    "\n",
    "]\n",
    " # data generation \n",
    "traingen = DataGenerator(data_loader.get_train_data,fraction = 0.1, repeat = 5)\n",
    "valid_gen = DataGenerator(data_loader.get_valid_data,fraction = 0.1, repeat = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_x shape (320, 100, 142)\n",
      "sample_y positive percents 0.765625\n",
      "time before model train 2020-01-04 01:44:13.812893\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, 142)         0         \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, None, 142)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                16800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 16,961\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6871 - accuracy: 0.5841 - val_loss: 0.5512 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55122, saving model to ./data/volume/local_test\\model-01-0.55.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6699 - accuracy: 0.6616 - val_loss: 0.5126 - val_accuracy: 0.7548\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55122 to 0.51262, saving model to ./data/volume/local_test\\model-02-0.51.hdf5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5ba2638595f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m model.train(traingen, valid_gen, epochs=10, valid_steps = 10, \n\u001b[1;32m---> 12\u001b[1;33m             step_epoch = 10, verbose=2, callbacks=callbacks, workers=-1)\n\u001b[0m",
      "\u001b[1;32mD:\\hack\\prism3\\prism\\docker\\src\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, traingen, valid_gen, epochs, valid_steps, step_epoch, verbose, callbacks, workers)\u001b[0m\n\u001b[0;32m     52\u001b[0m     self.model.fit_generator(generator= traingen, validation_data=valid_gen,\n\u001b[0;32m     53\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                     verbose=verbose, callbacks=callbacks,workers=workers)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36miter_sequence_infinite\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m    591\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-ba5595b457c5>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"there is non match\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     81\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_x,sample_y = traingen.__getitem__(1)\n",
    "print(\"sample_x shape\", sample_x.shape)\n",
    "print(\"sample_y positive percents\", sample_y.sum()/len(sample_y))\n",
    "\n",
    "print(\"time before model train\",datetime.datetime.now())\n",
    "model = SimpleRNNModel(shape=sample_x.shape[2])\n",
    "del sample_x #memory save \n",
    "\n",
    "# model train \n",
    "\n",
    "model.train(traingen, valid_gen, epochs=10, valid_steps = 10, \n",
    "            step_epoch = 10, verbose=2, callbacks=callbacks, workers=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
