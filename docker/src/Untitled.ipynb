{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두번째 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel,Autoencoder\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start\n",
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.004286766052246094\n",
      "data_loader extract_person time: 0.0033299922943115234\n",
      "data_loader extract_condition time: 0.002460002899169922\n",
      "data_loader extract_measurement time: 0.020641565322875977\n",
      "data_loader groupby_hour_condition time: 0.0058901309967041016\n",
      "data_loader groupby_hour_measurement time: 0.41345810890197754\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "measurement_df shape:  (402, 312)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "task_log_path = os.path.join(log_path, task_id)\n",
    "\n",
    "if not os.path.exists(task_path):\n",
    "  os.mkdir(task_path)\n",
    "if not os.path.exists(log_path):\n",
    "  os.mkdir(log_path)\n",
    "if not os.path.exists(task_log_path):\n",
    "  os.mkdir(task_log_path)\n",
    "\n",
    "print(\"Train Start\")\n",
    "\n",
    "#Autoencoding\n",
    "\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         autoencoder =True,\n",
    "                         task_path=task_path)\n",
    "\n",
    "auto_model = Autoencoder(data_loader.train_measure)\n",
    "print(\"measurement_df shape: \", data_loader.train_measure.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "\n",
      "Epoch 00001: saving model to ../../data/volume/local_test/encoder-01-219.883800.hdf5\n",
      "\n",
      "Epoch 00002: saving model to ../../data/volume/local_test/encoder-02-217.612148.hdf5\n",
      "\n",
      "Epoch 00003: saving model to ../../data/volume/local_test/encoder-03-215.026174.hdf5\n",
      "\n",
      "Epoch 00004: saving model to ../../data/volume/local_test/encoder-04-212.331903.hdf5\n",
      "\n",
      "Epoch 00005: saving model to ../../data/volume/local_test/encoder-05-209.651591.hdf5\n",
      "\n",
      "Epoch 00006: saving model to ../../data/volume/local_test/encoder-06-207.005978.hdf5\n",
      "\n",
      "Epoch 00007: saving model to ../../data/volume/local_test/encoder-07-204.499911.hdf5\n",
      "\n",
      "Epoch 00008: saving model to ../../data/volume/local_test/encoder-08-202.157562.hdf5\n",
      "\n",
      "Epoch 00009: saving model to ../../data/volume/local_test/encoder-09-199.951162.hdf5\n",
      "\n",
      "Epoch 00010: saving model to ../../data/volume/local_test/encoder-10-197.873374.hdf5\n",
      "\n",
      "Epoch 00011: saving model to ../../data/volume/local_test/encoder-11-195.895397.hdf5\n",
      "\n",
      "Epoch 00012: saving model to ../../data/volume/local_test/encoder-12-194.012621.hdf5\n",
      "\n",
      "Epoch 00013: saving model to ../../data/volume/local_test/encoder-13-192.192470.hdf5\n",
      "\n",
      "Epoch 00014: saving model to ../../data/volume/local_test/encoder-14-190.405839.hdf5\n",
      "\n",
      "Epoch 00015: saving model to ../../data/volume/local_test/encoder-15-188.685149.hdf5\n",
      "\n",
      "Epoch 00016: saving model to ../../data/volume/local_test/encoder-16-187.017353.hdf5\n",
      "\n",
      "Epoch 00017: saving model to ../../data/volume/local_test/encoder-17-185.393808.hdf5\n",
      "\n",
      "Epoch 00018: saving model to ../../data/volume/local_test/encoder-18-183.804859.hdf5\n",
      "\n",
      "Epoch 00019: saving model to ../../data/volume/local_test/encoder-19-182.235305.hdf5\n",
      "\n",
      "Epoch 00020: saving model to ../../data/volume/local_test/encoder-20-180.733344.hdf5\n",
      "\n",
      "Epoch 00021: saving model to ../../data/volume/local_test/encoder-21-179.232661.hdf5\n",
      "\n",
      "Epoch 00022: saving model to ../../data/volume/local_test/encoder-22-177.778429.hdf5\n",
      "\n",
      "Epoch 00023: saving model to ../../data/volume/local_test/encoder-23-176.362874.hdf5\n",
      "\n",
      "Epoch 00024: saving model to ../../data/volume/local_test/encoder-24-174.975781.hdf5\n",
      "\n",
      "Epoch 00025: saving model to ../../data/volume/local_test/encoder-25-173.584434.hdf5\n",
      "\n",
      "Epoch 00026: saving model to ../../data/volume/local_test/encoder-26-172.224733.hdf5\n",
      "\n",
      "Epoch 00027: saving model to ../../data/volume/local_test/encoder-27-170.893421.hdf5\n",
      "\n",
      "Epoch 00028: saving model to ../../data/volume/local_test/encoder-28-169.571165.hdf5\n",
      "\n",
      "Epoch 00029: saving model to ../../data/volume/local_test/encoder-29-168.294092.hdf5\n",
      "\n",
      "Epoch 00030: saving model to ../../data/volume/local_test/encoder-30-167.030409.hdf5\n",
      "\n",
      "Epoch 00031: saving model to ../../data/volume/local_test/encoder-31-165.772505.hdf5\n",
      "\n",
      "Epoch 00032: saving model to ../../data/volume/local_test/encoder-32-164.533870.hdf5\n",
      "\n",
      "Epoch 00033: saving model to ../../data/volume/local_test/encoder-33-163.348427.hdf5\n",
      "\n",
      "Epoch 00034: saving model to ../../data/volume/local_test/encoder-34-162.151130.hdf5\n",
      "\n",
      "Epoch 00035: saving model to ../../data/volume/local_test/encoder-35-161.004763.hdf5\n",
      "\n",
      "Epoch 00036: saving model to ../../data/volume/local_test/encoder-36-159.849115.hdf5\n",
      "\n",
      "Epoch 00037: saving model to ../../data/volume/local_test/encoder-37-158.706489.hdf5\n",
      "\n",
      "Epoch 00038: saving model to ../../data/volume/local_test/encoder-38-157.547362.hdf5\n",
      "\n",
      "Epoch 00039: saving model to ../../data/volume/local_test/encoder-39-156.448507.hdf5\n",
      "\n",
      "Epoch 00040: saving model to ../../data/volume/local_test/encoder-40-155.353503.hdf5\n",
      "\n",
      "Epoch 00041: saving model to ../../data/volume/local_test/encoder-41-154.257358.hdf5\n",
      "\n",
      "Epoch 00042: saving model to ../../data/volume/local_test/encoder-42-153.203737.hdf5\n",
      "\n",
      "Epoch 00043: saving model to ../../data/volume/local_test/encoder-43-152.151634.hdf5\n",
      "\n",
      "Epoch 00044: saving model to ../../data/volume/local_test/encoder-44-151.093783.hdf5\n",
      "\n",
      "Epoch 00045: saving model to ../../data/volume/local_test/encoder-45-150.062647.hdf5\n",
      "\n",
      "Epoch 00046: saving model to ../../data/volume/local_test/encoder-46-149.077634.hdf5\n",
      "\n",
      "Epoch 00047: saving model to ../../data/volume/local_test/encoder-47-148.053790.hdf5\n",
      "\n",
      "Epoch 00048: saving model to ../../data/volume/local_test/encoder-48-147.057178.hdf5\n",
      "\n",
      "Epoch 00049: saving model to ../../data/volume/local_test/encoder-49-146.079918.hdf5\n",
      "\n",
      "Epoch 00050: saving model to ../../data/volume/local_test/encoder-50-145.124167.hdf5\n",
      "\n",
      "Epoch 00051: saving model to ../../data/volume/local_test/encoder-51-144.182446.hdf5\n",
      "\n",
      "Epoch 00052: saving model to ../../data/volume/local_test/encoder-52-143.252468.hdf5\n",
      "\n",
      "Epoch 00053: saving model to ../../data/volume/local_test/encoder-53-142.267585.hdf5\n",
      "\n",
      "Epoch 00054: saving model to ../../data/volume/local_test/encoder-54-141.350066.hdf5\n",
      "\n",
      "Epoch 00055: saving model to ../../data/volume/local_test/encoder-55-140.464673.hdf5\n",
      "\n",
      "Epoch 00056: saving model to ../../data/volume/local_test/encoder-56-139.529009.hdf5\n",
      "\n",
      "Epoch 00057: saving model to ../../data/volume/local_test/encoder-57-138.666349.hdf5\n",
      "\n",
      "Epoch 00058: saving model to ../../data/volume/local_test/encoder-58-137.819297.hdf5\n",
      "\n",
      "Epoch 00059: saving model to ../../data/volume/local_test/encoder-59-136.909386.hdf5\n",
      "\n",
      "Epoch 00060: saving model to ../../data/volume/local_test/encoder-60-136.020783.hdf5\n",
      "\n",
      "Epoch 00061: saving model to ../../data/volume/local_test/encoder-61-135.141153.hdf5\n",
      "\n",
      "Epoch 00062: saving model to ../../data/volume/local_test/encoder-62-134.248442.hdf5\n",
      "\n",
      "Epoch 00063: saving model to ../../data/volume/local_test/encoder-63-133.417530.hdf5\n",
      "\n",
      "Epoch 00064: saving model to ../../data/volume/local_test/encoder-64-132.585707.hdf5\n",
      "\n",
      "Epoch 00065: saving model to ../../data/volume/local_test/encoder-65-131.739755.hdf5\n",
      "\n",
      "Epoch 00066: saving model to ../../data/volume/local_test/encoder-66-130.915474.hdf5\n",
      "\n",
      "Epoch 00067: saving model to ../../data/volume/local_test/encoder-67-130.131354.hdf5\n",
      "\n",
      "Epoch 00068: saving model to ../../data/volume/local_test/encoder-68-129.300530.hdf5\n",
      "\n",
      "Epoch 00069: saving model to ../../data/volume/local_test/encoder-69-128.465165.hdf5\n",
      "\n",
      "Epoch 00070: saving model to ../../data/volume/local_test/encoder-70-127.684555.hdf5\n",
      "\n",
      "Epoch 00071: saving model to ../../data/volume/local_test/encoder-71-126.898477.hdf5\n",
      "\n",
      "Epoch 00072: saving model to ../../data/volume/local_test/encoder-72-126.134813.hdf5\n",
      "\n",
      "Epoch 00073: saving model to ../../data/volume/local_test/encoder-73-125.354131.hdf5\n",
      "\n",
      "Epoch 00074: saving model to ../../data/volume/local_test/encoder-74-124.582465.hdf5\n",
      "\n",
      "Epoch 00075: saving model to ../../data/volume/local_test/encoder-75-123.785540.hdf5\n",
      "\n",
      "Epoch 00076: saving model to ../../data/volume/local_test/encoder-76-122.963151.hdf5\n",
      "\n",
      "Epoch 00077: saving model to ../../data/volume/local_test/encoder-77-122.254749.hdf5\n",
      "\n",
      "Epoch 00078: saving model to ../../data/volume/local_test/encoder-78-121.525743.hdf5\n",
      "\n",
      "Epoch 00079: saving model to ../../data/volume/local_test/encoder-79-120.760659.hdf5\n",
      "\n",
      "Epoch 00080: saving model to ../../data/volume/local_test/encoder-80-120.040926.hdf5\n",
      "\n",
      "Epoch 00081: saving model to ../../data/volume/local_test/encoder-81-119.328165.hdf5\n",
      "\n",
      "Epoch 00082: saving model to ../../data/volume/local_test/encoder-82-118.602706.hdf5\n",
      "\n",
      "Epoch 00083: saving model to ../../data/volume/local_test/encoder-83-117.891999.hdf5\n",
      "\n",
      "Epoch 00084: saving model to ../../data/volume/local_test/encoder-84-117.225284.hdf5\n",
      "\n",
      "Epoch 00085: saving model to ../../data/volume/local_test/encoder-85-116.502496.hdf5\n",
      "\n",
      "Epoch 00086: saving model to ../../data/volume/local_test/encoder-86-115.828241.hdf5\n",
      "\n",
      "Epoch 00087: saving model to ../../data/volume/local_test/encoder-87-115.122140.hdf5\n",
      "\n",
      "Epoch 00088: saving model to ../../data/volume/local_test/encoder-88-114.426097.hdf5\n",
      "\n",
      "Epoch 00089: saving model to ../../data/volume/local_test/encoder-89-113.752626.hdf5\n",
      "\n",
      "Epoch 00090: saving model to ../../data/volume/local_test/encoder-90-113.094496.hdf5\n",
      "\n",
      "Epoch 00091: saving model to ../../data/volume/local_test/encoder-91-112.434541.hdf5\n",
      "\n",
      "Epoch 00092: saving model to ../../data/volume/local_test/encoder-92-111.841686.hdf5\n",
      "\n",
      "Epoch 00093: saving model to ../../data/volume/local_test/encoder-93-111.185358.hdf5\n",
      "\n",
      "Epoch 00094: saving model to ../../data/volume/local_test/encoder-94-110.549215.hdf5\n",
      "\n",
      "Epoch 00095: saving model to ../../data/volume/local_test/encoder-95-109.876166.hdf5\n",
      "\n",
      "Epoch 00096: saving model to ../../data/volume/local_test/encoder-96-109.295483.hdf5\n",
      "\n",
      "Epoch 00097: saving model to ../../data/volume/local_test/encoder-97-108.685720.hdf5\n",
      "\n",
      "Epoch 00098: saving model to ../../data/volume/local_test/encoder-98-107.974755.hdf5\n",
      "\n",
      "Epoch 00099: saving model to ../../data/volume/local_test/encoder-99-107.359128.hdf5\n",
      "\n",
      "Epoch 00100: saving model to ../../data/volume/local_test/encoder-100-106.719855.hdf5\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "callbacks1 = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'encoder-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                                   verbose=0, mode='auto')\n",
    "]\n",
    "hist = auto_model.train(data_loader.train_measure,data_loader.valid_measure,\n",
    "            verbose=0,\n",
    "            epochs=100, batch_size=32,\n",
    "            callbacks=callbacks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.007924556732177734\n",
      "data_loader extract_person time: 0.0033881664276123047\n",
      "data_loader extract_condition time: 0.0025289058685302734\n",
      "data_loader extract_measurement time: 0.020534753799438477\n",
      "data_loader groupby_hour_condition time: 0.005887269973754883\n",
      "data_loader groupby_hour_measurement time: 0.4714365005493164\n",
      "../../data/volume/local_test/encoder-99-98.576439.hdf5\n",
      "len of keylist:  674 shape of measurement:  (503, 35)\n",
      "data_loader make_person_sequence time: 0.02490830421447754\n",
      "X (674,)\n",
      "Y (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.05288243293762207\n",
      "data_loader split_data time: 0.00860905647277832\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         autoencoder =False,\n",
    "                         task_path=task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674, 128, 49)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, data_loader.train_x.shape[2]))\n",
    "x = layers.Masking(mask_value=0)(model_input)\n",
    "x = layers.GRU(32, activation='tanh',\n",
    "               return_sequences=False)(model_input)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "model_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "optimizer = adam(lr=0.001)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = task_path + \"/rnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 674 samples, validate on 674 samples\n",
      "Epoch 1/10\n",
      "674/674 [==============================] - 0s 386us/step - loss: 0.5427 - acc: 0.8858 - val_loss: 0.5244 - val_acc: 0.8858\n",
      "Epoch 2/10\n",
      "674/674 [==============================] - 0s 361us/step - loss: 0.5341 - acc: 0.8858 - val_loss: 0.5215 - val_acc: 0.8858\n",
      "Epoch 3/10\n",
      "674/674 [==============================] - 0s 344us/step - loss: 0.5401 - acc: 0.8843 - val_loss: 0.5187 - val_acc: 0.8858\n",
      "Epoch 4/10\n",
      "674/674 [==============================] - 0s 346us/step - loss: 0.5254 - acc: 0.8872 - val_loss: 0.5158 - val_acc: 0.8858\n",
      "Epoch 5/10\n",
      "674/674 [==============================] - 0s 354us/step - loss: 0.5245 - acc: 0.8843 - val_loss: 0.5128 - val_acc: 0.8858\n",
      "Epoch 6/10\n",
      "674/674 [==============================] - 0s 369us/step - loss: 0.5275 - acc: 0.8828 - val_loss: 0.5096 - val_acc: 0.8858\n",
      "Epoch 7/10\n",
      "674/674 [==============================] - 0s 342us/step - loss: 0.5272 - acc: 0.8858 - val_loss: 0.5063 - val_acc: 0.8858\n",
      "Epoch 8/10\n",
      "674/674 [==============================] - 0s 338us/step - loss: 0.5158 - acc: 0.8858 - val_loss: 0.5029 - val_acc: 0.8858\n",
      "Epoch 9/10\n",
      "674/674 [==============================] - 0s 349us/step - loss: 0.5115 - acc: 0.8843 - val_loss: 0.4993 - val_acc: 0.8858\n",
      "Epoch 10/10\n",
      "674/674 [==============================] - 0s 340us/step - loss: 0.5168 - acc: 0.8813 - val_loss: 0.4955 - val_acc: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5083ca590>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(name, monitor='val_loss', verbose=0,\n",
    "                                         save_best_only=True, mode='min')]\n",
    "model.fit(data_loader.get_train_data()[0], data_loader.get_train_data()[1],\n",
    "            verbose=1,\n",
    "            epochs=10, batch_size=1000,\n",
    "          validation_data = data_loader.get_valid_data(),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tloss\tacc\tv_loss\tv_acc\n",
      "1\tnan\t0.9608\tnan\t0.9683\n",
      "2\tnan\t0.9714\tnan\t0.9683\n",
      "3\tnan\t0.9714\tnan\t0.9683\n",
      "4\tnan\t0.9714\tnan\t0.9683\n",
      "5\tnan\t0.9714\tnan\t0.9683\n",
      "6\tnan\t0.9714\tnan\t0.9683\n",
      "7\tnan\t0.9714\tnan\t0.9683\n",
      "8\tnan\t0.9714\tnan\t0.9683\n",
      "9\tnan\t0.9714\tnan\t0.9683\n",
      "10\tnan\t0.9714\tnan\t0.9683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keys = hist.history.keys()\n",
    "print_keys = [key.replace('val_', 'v_') for key in (['epoch'] + list(keys))]\n",
    "print(\"\\t\".join(print_keys))\n",
    "for idx in range(len(hist.history['loss'])):\n",
    "  log_list = [str(idx+1)]\n",
    "  for key in keys:\n",
    "    log_list.append(\"%.4f\" % hist.history[key][idx])\n",
    "  \n",
    "  print(\"\\t\".join(log_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "from measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "\n",
    "condition_list = [\n",
    "  'A49.0',\n",
    "  'B34.9',\n",
    "  'B95.6',\n",
    "  'D18.0',\n",
    "  'D22.5',\n",
    "  'D22.9',\n",
    "  'D64.9',\n",
    "  'D75.8',\n",
    "  'E03.1',\n",
    "  'E03.8',\n",
    "  'E03.9',\n",
    "  'E22.2',\n",
    "  'E27.4',\n",
    "  'E55.0',\n",
    "  'E63.9',\n",
    "  'E80.7',\n",
    "  'E83.5',\n",
    "  'E87.1',\n",
    "  'E87.2',\n",
    "  'E88.9',\n",
    "  'F19.0',\n",
    "  'G00.2',\n",
    "  'G00.8',\n",
    "  'G25.3',\n",
    "  'G91.8',\n",
    "  'G93.8',\n",
    "  'H35.1',\n",
    "  'H65.9',\n",
    "  'H66.9',\n",
    "  'H90.2',\n",
    "  'H91.9',\n",
    "  'H93.2',\n",
    "  'I27.2',\n",
    "  'I28.8',\n",
    "  'I47.1',\n",
    "  'I50.9',\n",
    "  'J06.9',\n",
    "  'J12.3',\n",
    "  'J18.9',\n",
    "  'J21.1',\n",
    "  'J21.9',\n",
    "  'J40',\n",
    "  'J93.9',\n",
    "  'J98.1',\n",
    "  'K21.9',\n",
    "  'K40.3',\n",
    "  'K40.9',\n",
    "  'K56.5',\n",
    "  'K59.0',\n",
    "  'K61.0',\n",
    "  'K91.4',\n",
    "  'K92.2',\n",
    "  'L05.9',\n",
    "  'L22',\n",
    "  'L74.3',\n",
    "  'L92.8',\n",
    "  'M43.62',\n",
    "  'M67.4',\n",
    "  'N13.3',\n",
    "  'N17.9',\n",
    "  'N43.3',\n",
    "  'N48.1',\n",
    "  'N83.2',\n",
    "  'N94.8',\n",
    "  'O31.2',\n",
    "  'P00.0',\n",
    "  'P00.8',\n",
    "  'P00.9',\n",
    "  'P01.0',\n",
    "  'P01.1',\n",
    "  'P01.2',\n",
    "  'P01.3',\n",
    "  'P01.5',\n",
    "  'P01.7',\n",
    "  'P02.0',\n",
    "  'P02.1',\n",
    "  'P02.4',\n",
    "  'P02.7',\n",
    "  'P03.4',\n",
    "  'P04.0',\n",
    "  'P05.1',\n",
    "  'P05.9',\n",
    "  'P07.0',\n",
    "  'P07.1',\n",
    "  'P07.2',\n",
    "  'P07.3',\n",
    "  'P08.1',\n",
    "  'P22.0',\n",
    "  'P22.9',\n",
    "  'P25.1',\n",
    "  'P26.1',\n",
    "  'P26.9',\n",
    "  'P27.1',\n",
    "  'P28.2',\n",
    "  'P28.4',\n",
    "  'P29.3',\n",
    "  'P29.8',\n",
    "  'P35.1',\n",
    "  'P52.0',\n",
    "  'P52.1',\n",
    "  'P52.2',\n",
    "  'P52.3',\n",
    "  'P52.6',\n",
    "  'P52.8',\n",
    "  'P52.9',\n",
    "  'P54.0',\n",
    "  'P54.3',\n",
    "  'P56.9',\n",
    "  'P59.0',\n",
    "  'P59.8',\n",
    "  'P59.9',\n",
    "  'P61.0',\n",
    "  'P61.2',\n",
    "  'P70.0',\n",
    "  'P70.4',\n",
    "  'P72.2',\n",
    "  'P74',\n",
    "  'P76.0',\n",
    "  'P78.1',\n",
    "  'P81.9',\n",
    "  'P90',\n",
    "  'P91.2',\n",
    "  'P91.7',\n",
    "  'P92.9',\n",
    "  'P94.2',\n",
    "  'P96.8',\n",
    "  'Q04.3',\n",
    "  'Q04.6',\n",
    "  'Q04.8',\n",
    "  'Q10.3',\n",
    "  'Q10.5',\n",
    "  'Q21.1',\n",
    "  'Q25.0',\n",
    "  'Q27.0',\n",
    "  'Q31.5',\n",
    "  'Q33.6',\n",
    "  'Q42.3',\n",
    "  'Q53.1',\n",
    "  'Q53.2',\n",
    "  'Q53.9',\n",
    "  'Q62.0',\n",
    "  'Q63.2',\n",
    "  'Q64.4',\n",
    "  'Q65.2',\n",
    "  'Q66.8',\n",
    "  'Q67.3',\n",
    "  'Q69.1',\n",
    "  'Q82.8',\n",
    "  'Q87.2',\n",
    "  'Q89.9',\n",
    "  'R00.1',\n",
    "  'R04.8',\n",
    "  'R05',\n",
    "  'R09.2',\n",
    "  'R09.3',\n",
    "  'R09.8',\n",
    "  'R11',\n",
    "  'R16.2',\n",
    "  'R21',\n",
    "  'R25.1',\n",
    "  'R34',\n",
    "  'R49.0',\n",
    "  'R50.9',\n",
    "  'R56.8',\n",
    "  'R62.0',\n",
    "  'R62.8',\n",
    "  'R68.1',\n",
    "  'R73.9',\n",
    "  'R94.6',\n",
    "  'S09.9',\n",
    "  'S36.4',\n",
    "  'T18.9',\n",
    "  'T81.3',\n",
    "  'U83.0',\n",
    "  'Z00.0',\n",
    "  'Z01.0',\n",
    "  'Z13.9',\n",
    "  'Z26.9',\n",
    "  'Z38.0',\n",
    "  'Z38.1',\n",
    "  'Z38.3',\n",
    "  'Z38.6'\n",
    "]\n",
    "\n",
    "label_ratio = 0.1\n",
    "\n",
    "person_cols = ['PERSON_ID', 'BIRTH_DATETIME', 'GENDER_SOURCE_VALUE']\n",
    "condition_cols = ['PERSON_ID', 'CONDITION_START_DATETIME', 'CONDITION_SOURCE_VALUE']\n",
    "measurement_cols = ['PERSON_ID', 'MEASUREMENT_DATETIME', 'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "outcome_cols = ['SUBJECT_ID', 'COHORT_START_DATE', 'COHORT_END_DATE', 'LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_person(n=3):\n",
    "  data = []\n",
    "  gender = ['M', 'F']\n",
    "  for i in range(n):\n",
    "    data.append((i, randomtimes(n=1)[0], gender[i % 2]))\n",
    "  return data\n",
    "\n",
    "def generate_condition(n=5):\n",
    "  return random.choices(condition_list, k=n)\n",
    "\n",
    "def generate_measurement(n=5):\n",
    "  data = []\n",
    "  for k, stats in MEASUREMENT_SOURCE_VALUE_STATS.items():\n",
    "    values = np.random.normal(loc=stats['AVERAGE'], scale=stats['STANDARD DEVIATION'], size=n)\n",
    "    \n",
    "    data.extend([(k, v) for v in values])\n",
    "  np.random.shuffle(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcome_dts(stime, etime):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt).replace(minute=00, second=00) + datetime.timedelta(hours=1)\n",
    "  etime = datetime.datetime.strptime(etime, frmt) + datetime.timedelta(hours=1)\n",
    "  data = pd.date_range(start=stime, end=etime, freq='H')\n",
    "   \n",
    "  return data\n",
    "\n",
    "\n",
    "def randomtimes(stime='2019-01-01 00:00:00', etime='2019-02-01 00:00:00', n=10):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt)\n",
    "  if etime is None:\n",
    "    etime = stime + datetime.timedelta(days=2)\n",
    "  else:\n",
    "    etime = datetime.datetime.strptime(etime, frmt)\n",
    "  td = etime - stime\n",
    "  dts = [(random.random() * td + stime).strftime(frmt) for _ in range(n)]\n",
    "  dts.sort()\n",
    "  return dts\n",
    "\n",
    "\n",
    "def generate_dev_data(n=3, n_cond= 5, n_msmt=5):\n",
    "  \n",
    "  cond_list = []\n",
    "  msmt_list = []\n",
    "  person_list = generate_person(n)\n",
    "  outcome_list = []\n",
    "  for person in person_list:\n",
    "    # 환자를 생성\n",
    "    i = person[0]\n",
    "    min_date = person[1]\n",
    "    max_date = person[1]\n",
    "    # 진단을 생성\n",
    "    for dt, cond in zip(randomtimes(stime=min_date, n=n_cond), generate_condition(n_cond)):\n",
    "      cond_list.append((i, dt, cond))\n",
    "      if dt > max_date: # 가장 나중날짜를 코호트 종료날짜로\n",
    "        max_date = dt\n",
    "      # 바이탈사인 날짜 생성. 측정날짜는 진단 날짜부터\n",
    "      msmt_dts = randomtimes(stime=dt, n=n_msmt * len(MEASUREMENT_SOURCE_VALUE_STATS))\n",
    "      for dt2, msmt in zip(msmt_dts, generate_measurement(n_msmt)):\n",
    "        msmt_list.append((i, dt2, msmt[0], msmt[1]))\n",
    "        if dt2 > max_date:\n",
    "          max_date = dt2 # 가장 나중날짜를 코호트 종료날짜로\n",
    "    \n",
    "    # 코호트 종료날짜 1시간단위로 롤링\n",
    "    end_dts = generate_outcome_dts(min_date, max_date)\n",
    "    start_dts = [min_date] * len(end_dts)\n",
    "    subject_ids = [i] * len(end_dts)\n",
    "    # 언발란스로 라벨 생성\n",
    "    labels = np.random.choice([0, 1], size=len(end_dts), p=[1-label_ratio, label_ratio])\n",
    "    outcome_list.extend(list(zip(subject_ids, start_dts, end_dts, labels)))\n",
    "\n",
    "  # 샘플 데이터프레임 생성\n",
    "  person_df = pd.DataFrame(person_list, columns=person_cols)\n",
    "  condition_df = pd.DataFrame(cond_list, columns=condition_cols).sort_values(condition_cols[:2])\n",
    "  measurement_df = pd.DataFrame(msmt_list, columns=measurement_cols).sort_values(measurement_cols[:2])\n",
    "  outcome_df = pd.DataFrame(outcome_list, columns=outcome_cols)\n",
    "\n",
    "  # 샘플 데이터프레임 저장\n",
    "  data_path = os.path.join('../..', 'data')\n",
    "  dev_data = 'dev'\n",
    "  csv_files = {\n",
    "    'person': f'{dev_data}PERSON_NICU.csv',\n",
    "    'condition': f'{dev_data}CONDITION_OCCURRENCE_NICU.csv',\n",
    "    'measurement': f'{dev_data}MEASUREMENT_NICU.csv',\n",
    "    'outcome': f'{dev_data}OUTCOME_COHORT.csv'\n",
    "  }\n",
    "  person_df.to_csv(os.path.join(data_path, 'train', csv_files['person']), index=False)\n",
    "  condition_df.to_csv(os.path.join(data_path, 'train', csv_files['condition']), index=False)\n",
    "  measurement_df.to_csv(os.path.join(data_path, 'train', csv_files['measurement']), index=False)\n",
    "  outcome_df.to_csv(os.path.join(data_path, 'train', csv_files['outcome']), index=False)\n",
    "\n",
    "  print()\n",
    "  print('generated data shapes')\n",
    "  print(person_df.shape)\n",
    "  print(condition_df.shape)\n",
    "  print(measurement_df.shape)\n",
    "  print(outcome_df.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = os.environ.get('DEV_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Start\n",
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.0045435428619384766\n",
      "data_loader extract_person time: 0.0035893917083740234\n",
      "data_loader extract_condition time: 0.002619028091430664\n",
      "data_loader extract_measurement time: 0.01977705955505371\n",
      "data_loader groupby_hour_condition time: 0.006977081298828125\n",
      "data_loader groupby_hour_measurement time: 1.7550389766693115\n",
      "len of keylist:  674 shape of measurement:  (489, 302)\n",
      "data_loader make_person_sequence time: 0.03047323226928711\n",
      "X (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.07169747352600098\n",
      "data_loader split_data time: 0.020588159561157227\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path='../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "\n",
    "print(\"Inference Start\")\n",
    "\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'test'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                         is_train=False)\n",
    "model = SimpleRNNModel(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ba75ad59f0f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_infer_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/prism/prism/docker/src/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, infer_x, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# threhshold\n",
    "f1_list = np.load(os.path.join(task_path, 'f1.npy'))\n",
    "thr_list = np.load(os.path.join(task_path, 'thr.npy'))\n",
    "thr = thr_list[np.argmax(f1_list)]\n",
    "\n",
    "y_key = data_loader.key\n",
    "y_pred = model.predict(data_loader.get_infer_data())\n",
    "y_pred = pd.DataFrame(y_pred, columns=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/volume/local_test/model-10-0.341793.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.load(task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
