{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두번째 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel,Autoencoder\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.017416000366210938\n",
      "data_loader extract_person time: 0.009947538375854492\n",
      "data_loader extract_condition time: 0.005637407302856445\n",
      "data_loader extract_measurement time: 0.01613450050354004\n",
      "data_loader groupby_hour_condition time: 0.00671839714050293\n",
      "data_loader groupby_hour_measurement time: 0.7709228992462158\n",
      "len of keylist:  674 shape of measurement:  (528, 603)\n",
      "data_loader make_person_sequence time: 0.045790910720825195\n",
      "X (674,)\n",
      "Y (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.06693887710571289\n",
      "data_loader split_data time: 0.0654911994934082\n",
      "train_x shape:  (674, 128, 617)\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "task_log_path = os.path.join(log_path, task_id)\n",
    "\n",
    "\n",
    "# auto_model.predict(data_loader.measure_auto)\n",
    "data_loader2 = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         autoencoder =False,\n",
    "                         task_path=task_path)\n",
    "\n",
    "model = SimpleRNNModel(data_loader2)\n",
    "print(\"train_x shape: \",data_loader2.train_x.shape)\n",
    "callbacks2 = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'model-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                                   verbose=0, mode='auto'),\n",
    "    TensorBoard(log_dir=task_log_path, write_graph=True)\n",
    "]\n",
    "\n",
    "# hist = model.train(data_loader2.get_train_data(), data_loader2.get_valid_data(),\n",
    "#             verbose=0,\n",
    "#             epochs=100, batch_size=200,\n",
    "#             callbacks=callbacks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38443, saving model to ../../data/volume/local_test/model-01-0.384434.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.38443\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.38443 to 0.38060, saving model to ../../data/volume/local_test/model-03-0.380598.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.38060 to 0.36623, saving model to ../../data/volume/local_test/model-04-0.366233.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.36623 to 0.35901, saving model to ../../data/volume/local_test/model-05-0.359015.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.35901 to 0.35694, saving model to ../../data/volume/local_test/model-06-0.356941.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.35694 to 0.35603, saving model to ../../data/volume/local_test/model-07-0.356032.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35603\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.35603\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.35603\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.35603\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35603 to 0.35317, saving model to ../../data/volume/local_test/model-12-0.353169.hdf5\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35317 to 0.35177, saving model to ../../data/volume/local_test/model-13-0.351771.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35177\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.35177\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.35177\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.35177\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.35177\n"
     ]
    }
   ],
   "source": [
    "hist = model.train(data_loader2.get_train_data(), data_loader2.get_valid_data(),\n",
    "            verbose=0,\n",
    "            epochs=100, batch_size=200,\n",
    "            callbacks=callbacks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = hist.history.keys()\n",
    "print_keys = [key.replace('val_', 'v_') for key in (['epoch'] + list(keys))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': [0.38443352852447804,\n",
       "  0.39611626175461606,\n",
       "  0.3805978841173189,\n",
       "  0.3662330159801404,\n",
       "  0.35901477543819554,\n",
       "  0.35694118741711806,\n",
       "  0.35603216157822654,\n",
       "  0.35740807210302494,\n",
       "  0.36263965180790386,\n",
       "  0.3656352678816821,\n",
       "  0.3595607239167131,\n",
       "  0.3531685559084578,\n",
       "  0.3517707932950481,\n",
       "  0.35300807668122763,\n",
       "  0.3571679612472432,\n",
       "  0.35545684737692246,\n",
       "  0.35437685976396327,\n",
       "  0.3523853397334011],\n",
       " 'val_acc': [0.8768545994065282,\n",
       "  0.8813056342679598,\n",
       "  0.8842729910191159,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914,\n",
       "  0.8857566691293914],\n",
       " 'loss': [0.41477337532651887,\n",
       "  0.39150682065182696,\n",
       "  0.3868849830210032,\n",
       "  0.38446937735781117,\n",
       "  0.364661066839886,\n",
       "  0.3773203904267942,\n",
       "  0.3791742315101341,\n",
       "  0.36793375820009927,\n",
       "  0.37276802413187676,\n",
       "  0.37827792551467965,\n",
       "  0.3700125406686913,\n",
       "  0.37212055426323093,\n",
       "  0.36675058345058903,\n",
       "  0.37610176267539003,\n",
       "  0.3642437457153988,\n",
       "  0.35774587497159355,\n",
       "  0.3667482795630435,\n",
       "  0.3570195453280157],\n",
       " 'acc': [0.8605341214454493,\n",
       "  0.8798219561576843,\n",
       "  0.8798219743751277,\n",
       "  0.8827893213985228,\n",
       "  0.8842730071141387,\n",
       "  0.8842729818219601,\n",
       "  0.8857566862856245,\n",
       "  0.8857566873468348,\n",
       "  0.8857566701906018,\n",
       "  0.885756667537576,\n",
       "  0.885756667537576,\n",
       "  0.8857566691293914,\n",
       "  0.8827893123782353,\n",
       "  0.8857566868162297,\n",
       "  0.8842729894273005,\n",
       "  0.8857566707212069,\n",
       "  0.8857566852244142,\n",
       "  0.8857566873468348]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tv_loss\tv_acc\tloss\tacc\n",
      "1\t0.3844\t0.8769\t0.4148\t0.8605\n",
      "2\t0.3961\t0.8813\t0.3915\t0.8798\n",
      "3\t0.3806\t0.8843\t0.3869\t0.8798\n",
      "4\t0.3662\t0.8858\t0.3845\t0.8828\n",
      "5\t0.3590\t0.8858\t0.3647\t0.8843\n",
      "6\t0.3569\t0.8858\t0.3773\t0.8843\n",
      "7\t0.3560\t0.8858\t0.3792\t0.8858\n",
      "8\t0.3574\t0.8858\t0.3679\t0.8858\n",
      "9\t0.3626\t0.8858\t0.3728\t0.8858\n",
      "10\t0.3656\t0.8858\t0.3783\t0.8858\n",
      "11\t0.3596\t0.8858\t0.3700\t0.8858\n",
      "12\t0.3532\t0.8858\t0.3721\t0.8858\n",
      "13\t0.3518\t0.8858\t0.3668\t0.8828\n",
      "14\t0.3530\t0.8858\t0.3761\t0.8858\n",
      "15\t0.3572\t0.8858\t0.3642\t0.8843\n",
      "16\t0.3555\t0.8858\t0.3577\t0.8858\n",
      "17\t0.3544\t0.8858\t0.3667\t0.8858\n",
      "18\t0.3524\t0.8858\t0.3570\t0.8858\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\".join(print_keys))\n",
    "\n",
    "for idx in range(len(hist.history['loss'])):\n",
    "  log_list = [str(idx+1)]\n",
    "  for key in keys:\n",
    "    log_list.append(\"%.4f\" % hist.history[key][idx])\n",
    "\n",
    "  print(\"\\t\".join(log_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tv_loss\tv_acc\tloss\tacc\n"
     ]
    }
   ],
   "source": [
    "print_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "callbacks1 = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'encoder-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=False,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    EarlyStopping(monitor='val_loss', min_delta=0, patience=5,\n",
    "                                   verbose=0, mode='auto')\n",
    "]\n",
    "hist = auto_model.train(data_loader.train_measure,data_loader.valid_measure,\n",
    "            verbose=0,\n",
    "            epochs=100, batch_size=32,\n",
    "            callbacks=callbacks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.007924556732177734\n",
      "data_loader extract_person time: 0.0033881664276123047\n",
      "data_loader extract_condition time: 0.0025289058685302734\n",
      "data_loader extract_measurement time: 0.020534753799438477\n",
      "data_loader groupby_hour_condition time: 0.005887269973754883\n",
      "data_loader groupby_hour_measurement time: 0.4714365005493164\n",
      "../../data/volume/local_test/encoder-99-98.576439.hdf5\n",
      "len of keylist:  674 shape of measurement:  (503, 35)\n",
      "data_loader make_person_sequence time: 0.02490830421447754\n",
      "X (674,)\n",
      "Y (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.05288243293762207\n",
      "data_loader split_data time: 0.00860905647277832\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         autoencoder =False,\n",
    "                         task_path=task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674, 128, 49)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, data_loader.train_x.shape[2]))\n",
    "x = layers.Masking(mask_value=0)(model_input)\n",
    "x = layers.GRU(32, activation='tanh',\n",
    "               return_sequences=False)(model_input)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "model_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "optimizer = adam(lr=0.001)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = task_path + \"/rnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 674 samples, validate on 674 samples\n",
      "Epoch 1/10\n",
      "674/674 [==============================] - 0s 386us/step - loss: 0.5427 - acc: 0.8858 - val_loss: 0.5244 - val_acc: 0.8858\n",
      "Epoch 2/10\n",
      "674/674 [==============================] - 0s 361us/step - loss: 0.5341 - acc: 0.8858 - val_loss: 0.5215 - val_acc: 0.8858\n",
      "Epoch 3/10\n",
      "674/674 [==============================] - 0s 344us/step - loss: 0.5401 - acc: 0.8843 - val_loss: 0.5187 - val_acc: 0.8858\n",
      "Epoch 4/10\n",
      "674/674 [==============================] - 0s 346us/step - loss: 0.5254 - acc: 0.8872 - val_loss: 0.5158 - val_acc: 0.8858\n",
      "Epoch 5/10\n",
      "674/674 [==============================] - 0s 354us/step - loss: 0.5245 - acc: 0.8843 - val_loss: 0.5128 - val_acc: 0.8858\n",
      "Epoch 6/10\n",
      "674/674 [==============================] - 0s 369us/step - loss: 0.5275 - acc: 0.8828 - val_loss: 0.5096 - val_acc: 0.8858\n",
      "Epoch 7/10\n",
      "674/674 [==============================] - 0s 342us/step - loss: 0.5272 - acc: 0.8858 - val_loss: 0.5063 - val_acc: 0.8858\n",
      "Epoch 8/10\n",
      "674/674 [==============================] - 0s 338us/step - loss: 0.5158 - acc: 0.8858 - val_loss: 0.5029 - val_acc: 0.8858\n",
      "Epoch 9/10\n",
      "674/674 [==============================] - 0s 349us/step - loss: 0.5115 - acc: 0.8843 - val_loss: 0.4993 - val_acc: 0.8858\n",
      "Epoch 10/10\n",
      "674/674 [==============================] - 0s 340us/step - loss: 0.5168 - acc: 0.8813 - val_loss: 0.4955 - val_acc: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5083ca590>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(name, monitor='val_loss', verbose=0,\n",
    "                                         save_best_only=True, mode='min')]\n",
    "model.fit(data_loader.get_train_data()[0], data_loader.get_train_data()[1],\n",
    "            verbose=1,\n",
    "            epochs=10, batch_size=1000,\n",
    "          validation_data = data_loader.get_valid_data(),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tloss\tacc\tv_loss\tv_acc\n",
      "1\tnan\t0.9608\tnan\t0.9683\n",
      "2\tnan\t0.9714\tnan\t0.9683\n",
      "3\tnan\t0.9714\tnan\t0.9683\n",
      "4\tnan\t0.9714\tnan\t0.9683\n",
      "5\tnan\t0.9714\tnan\t0.9683\n",
      "6\tnan\t0.9714\tnan\t0.9683\n",
      "7\tnan\t0.9714\tnan\t0.9683\n",
      "8\tnan\t0.9714\tnan\t0.9683\n",
      "9\tnan\t0.9714\tnan\t0.9683\n",
      "10\tnan\t0.9714\tnan\t0.9683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keys = hist.history.keys()\n",
    "print_keys = [key.replace('val_', 'v_') for key in (['epoch'] + list(keys))]\n",
    "print(\"\\t\".join(print_keys))\n",
    "for idx in range(len(hist.history['loss'])):\n",
    "  log_list = [str(idx+1)]\n",
    "  for key in keys:\n",
    "    log_list.append(\"%.4f\" % hist.history[key][idx])\n",
    "  \n",
    "  print(\"\\t\".join(log_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "from measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "\n",
    "condition_list = [\n",
    "  'A49.0',\n",
    "  'B34.9',\n",
    "  'B95.6',\n",
    "  'D18.0',\n",
    "  'D22.5',\n",
    "  'D22.9',\n",
    "  'D64.9',\n",
    "  'D75.8',\n",
    "  'E03.1',\n",
    "  'E03.8',\n",
    "  'E03.9',\n",
    "  'E22.2',\n",
    "  'E27.4',\n",
    "  'E55.0',\n",
    "  'E63.9',\n",
    "  'E80.7',\n",
    "  'E83.5',\n",
    "  'E87.1',\n",
    "  'E87.2',\n",
    "  'E88.9',\n",
    "  'F19.0',\n",
    "  'G00.2',\n",
    "  'G00.8',\n",
    "  'G25.3',\n",
    "  'G91.8',\n",
    "  'G93.8',\n",
    "  'H35.1',\n",
    "  'H65.9',\n",
    "  'H66.9',\n",
    "  'H90.2',\n",
    "  'H91.9',\n",
    "  'H93.2',\n",
    "  'I27.2',\n",
    "  'I28.8',\n",
    "  'I47.1',\n",
    "  'I50.9',\n",
    "  'J06.9',\n",
    "  'J12.3',\n",
    "  'J18.9',\n",
    "  'J21.1',\n",
    "  'J21.9',\n",
    "  'J40',\n",
    "  'J93.9',\n",
    "  'J98.1',\n",
    "  'K21.9',\n",
    "  'K40.3',\n",
    "  'K40.9',\n",
    "  'K56.5',\n",
    "  'K59.0',\n",
    "  'K61.0',\n",
    "  'K91.4',\n",
    "  'K92.2',\n",
    "  'L05.9',\n",
    "  'L22',\n",
    "  'L74.3',\n",
    "  'L92.8',\n",
    "  'M43.62',\n",
    "  'M67.4',\n",
    "  'N13.3',\n",
    "  'N17.9',\n",
    "  'N43.3',\n",
    "  'N48.1',\n",
    "  'N83.2',\n",
    "  'N94.8',\n",
    "  'O31.2',\n",
    "  'P00.0',\n",
    "  'P00.8',\n",
    "  'P00.9',\n",
    "  'P01.0',\n",
    "  'P01.1',\n",
    "  'P01.2',\n",
    "  'P01.3',\n",
    "  'P01.5',\n",
    "  'P01.7',\n",
    "  'P02.0',\n",
    "  'P02.1',\n",
    "  'P02.4',\n",
    "  'P02.7',\n",
    "  'P03.4',\n",
    "  'P04.0',\n",
    "  'P05.1',\n",
    "  'P05.9',\n",
    "  'P07.0',\n",
    "  'P07.1',\n",
    "  'P07.2',\n",
    "  'P07.3',\n",
    "  'P08.1',\n",
    "  'P22.0',\n",
    "  'P22.9',\n",
    "  'P25.1',\n",
    "  'P26.1',\n",
    "  'P26.9',\n",
    "  'P27.1',\n",
    "  'P28.2',\n",
    "  'P28.4',\n",
    "  'P29.3',\n",
    "  'P29.8',\n",
    "  'P35.1',\n",
    "  'P52.0',\n",
    "  'P52.1',\n",
    "  'P52.2',\n",
    "  'P52.3',\n",
    "  'P52.6',\n",
    "  'P52.8',\n",
    "  'P52.9',\n",
    "  'P54.0',\n",
    "  'P54.3',\n",
    "  'P56.9',\n",
    "  'P59.0',\n",
    "  'P59.8',\n",
    "  'P59.9',\n",
    "  'P61.0',\n",
    "  'P61.2',\n",
    "  'P70.0',\n",
    "  'P70.4',\n",
    "  'P72.2',\n",
    "  'P74',\n",
    "  'P76.0',\n",
    "  'P78.1',\n",
    "  'P81.9',\n",
    "  'P90',\n",
    "  'P91.2',\n",
    "  'P91.7',\n",
    "  'P92.9',\n",
    "  'P94.2',\n",
    "  'P96.8',\n",
    "  'Q04.3',\n",
    "  'Q04.6',\n",
    "  'Q04.8',\n",
    "  'Q10.3',\n",
    "  'Q10.5',\n",
    "  'Q21.1',\n",
    "  'Q25.0',\n",
    "  'Q27.0',\n",
    "  'Q31.5',\n",
    "  'Q33.6',\n",
    "  'Q42.3',\n",
    "  'Q53.1',\n",
    "  'Q53.2',\n",
    "  'Q53.9',\n",
    "  'Q62.0',\n",
    "  'Q63.2',\n",
    "  'Q64.4',\n",
    "  'Q65.2',\n",
    "  'Q66.8',\n",
    "  'Q67.3',\n",
    "  'Q69.1',\n",
    "  'Q82.8',\n",
    "  'Q87.2',\n",
    "  'Q89.9',\n",
    "  'R00.1',\n",
    "  'R04.8',\n",
    "  'R05',\n",
    "  'R09.2',\n",
    "  'R09.3',\n",
    "  'R09.8',\n",
    "  'R11',\n",
    "  'R16.2',\n",
    "  'R21',\n",
    "  'R25.1',\n",
    "  'R34',\n",
    "  'R49.0',\n",
    "  'R50.9',\n",
    "  'R56.8',\n",
    "  'R62.0',\n",
    "  'R62.8',\n",
    "  'R68.1',\n",
    "  'R73.9',\n",
    "  'R94.6',\n",
    "  'S09.9',\n",
    "  'S36.4',\n",
    "  'T18.9',\n",
    "  'T81.3',\n",
    "  'U83.0',\n",
    "  'Z00.0',\n",
    "  'Z01.0',\n",
    "  'Z13.9',\n",
    "  'Z26.9',\n",
    "  'Z38.0',\n",
    "  'Z38.1',\n",
    "  'Z38.3',\n",
    "  'Z38.6'\n",
    "]\n",
    "\n",
    "label_ratio = 0.1\n",
    "\n",
    "person_cols = ['PERSON_ID', 'BIRTH_DATETIME', 'GENDER_SOURCE_VALUE']\n",
    "condition_cols = ['PERSON_ID', 'CONDITION_START_DATETIME', 'CONDITION_SOURCE_VALUE']\n",
    "measurement_cols = ['PERSON_ID', 'MEASUREMENT_DATETIME', 'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "outcome_cols = ['SUBJECT_ID', 'COHORT_START_DATE', 'COHORT_END_DATE', 'LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_person(n=3):\n",
    "  data = []\n",
    "  gender = ['M', 'F']\n",
    "  for i in range(n):\n",
    "    data.append((i, randomtimes(n=1)[0], gender[i % 2]))\n",
    "  return data\n",
    "\n",
    "def generate_condition(n=5):\n",
    "  return random.choices(condition_list, k=n)\n",
    "\n",
    "def generate_measurement(n=5):\n",
    "  data = []\n",
    "  for k, stats in MEASUREMENT_SOURCE_VALUE_STATS.items():\n",
    "    values = np.random.normal(loc=stats['AVERAGE'], scale=stats['STANDARD DEVIATION'], size=n)\n",
    "    \n",
    "    data.extend([(k, v) for v in values])\n",
    "  np.random.shuffle(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcome_dts(stime, etime):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt).replace(minute=00, second=00) + datetime.timedelta(hours=1)\n",
    "  etime = datetime.datetime.strptime(etime, frmt) + datetime.timedelta(hours=1)\n",
    "  data = pd.date_range(start=stime, end=etime, freq='H')\n",
    "   \n",
    "  return data\n",
    "\n",
    "\n",
    "def randomtimes(stime='2019-01-01 00:00:00', etime='2019-02-01 00:00:00', n=10):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt)\n",
    "  if etime is None:\n",
    "    etime = stime + datetime.timedelta(days=2)\n",
    "  else:\n",
    "    etime = datetime.datetime.strptime(etime, frmt)\n",
    "  td = etime - stime\n",
    "  dts = [(random.random() * td + stime).strftime(frmt) for _ in range(n)]\n",
    "  dts.sort()\n",
    "  return dts\n",
    "\n",
    "\n",
    "def generate_dev_data(n=3, n_cond= 5, n_msmt=5):\n",
    "  \n",
    "  cond_list = []\n",
    "  msmt_list = []\n",
    "  person_list = generate_person(n)\n",
    "  outcome_list = []\n",
    "  for person in person_list:\n",
    "    # 환자를 생성\n",
    "    i = person[0]\n",
    "    min_date = person[1]\n",
    "    max_date = person[1]\n",
    "    # 진단을 생성\n",
    "    for dt, cond in zip(randomtimes(stime=min_date, n=n_cond), generate_condition(n_cond)):\n",
    "      cond_list.append((i, dt, cond))\n",
    "      if dt > max_date: # 가장 나중날짜를 코호트 종료날짜로\n",
    "        max_date = dt\n",
    "      # 바이탈사인 날짜 생성. 측정날짜는 진단 날짜부터\n",
    "      msmt_dts = randomtimes(stime=dt, n=n_msmt * len(MEASUREMENT_SOURCE_VALUE_STATS))\n",
    "      for dt2, msmt in zip(msmt_dts, generate_measurement(n_msmt)):\n",
    "        msmt_list.append((i, dt2, msmt[0], msmt[1]))\n",
    "        if dt2 > max_date:\n",
    "          max_date = dt2 # 가장 나중날짜를 코호트 종료날짜로\n",
    "    \n",
    "    # 코호트 종료날짜 1시간단위로 롤링\n",
    "    end_dts = generate_outcome_dts(min_date, max_date)\n",
    "    start_dts = [min_date] * len(end_dts)\n",
    "    subject_ids = [i] * len(end_dts)\n",
    "    # 언발란스로 라벨 생성\n",
    "    labels = np.random.choice([0, 1], size=len(end_dts), p=[1-label_ratio, label_ratio])\n",
    "    outcome_list.extend(list(zip(subject_ids, start_dts, end_dts, labels)))\n",
    "\n",
    "  # 샘플 데이터프레임 생성\n",
    "  person_df = pd.DataFrame(person_list, columns=person_cols)\n",
    "  condition_df = pd.DataFrame(cond_list, columns=condition_cols).sort_values(condition_cols[:2])\n",
    "  measurement_df = pd.DataFrame(msmt_list, columns=measurement_cols).sort_values(measurement_cols[:2])\n",
    "  outcome_df = pd.DataFrame(outcome_list, columns=outcome_cols)\n",
    "\n",
    "  # 샘플 데이터프레임 저장\n",
    "  data_path = os.path.join('../..', 'data')\n",
    "  dev_data = 'dev'\n",
    "  csv_files = {\n",
    "    'person': f'{dev_data}PERSON_NICU.csv',\n",
    "    'condition': f'{dev_data}CONDITION_OCCURRENCE_NICU.csv',\n",
    "    'measurement': f'{dev_data}MEASUREMENT_NICU.csv',\n",
    "    'outcome': f'{dev_data}OUTCOME_COHORT.csv'\n",
    "  }\n",
    "  person_df.to_csv(os.path.join(data_path, 'train', csv_files['person']), index=False)\n",
    "  condition_df.to_csv(os.path.join(data_path, 'train', csv_files['condition']), index=False)\n",
    "  measurement_df.to_csv(os.path.join(data_path, 'train', csv_files['measurement']), index=False)\n",
    "  outcome_df.to_csv(os.path.join(data_path, 'train', csv_files['outcome']), index=False)\n",
    "\n",
    "  print()\n",
    "  print('generated data shapes')\n",
    "  print(person_df.shape)\n",
    "  print(condition_df.shape)\n",
    "  print(measurement_df.shape)\n",
    "  print(outcome_df.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = os.environ.get('DEV_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Start\n",
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.0045435428619384766\n",
      "data_loader extract_person time: 0.0035893917083740234\n",
      "data_loader extract_condition time: 0.002619028091430664\n",
      "data_loader extract_measurement time: 0.01977705955505371\n",
      "data_loader groupby_hour_condition time: 0.006977081298828125\n",
      "data_loader groupby_hour_measurement time: 1.7550389766693115\n",
      "len of keylist:  674 shape of measurement:  (489, 302)\n",
      "data_loader make_person_sequence time: 0.03047323226928711\n",
      "X (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.07169747352600098\n",
      "data_loader split_data time: 0.020588159561157227\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path='../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "\n",
    "print(\"Inference Start\")\n",
    "\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'test'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                         is_train=False)\n",
    "model = SimpleRNNModel(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ba75ad59f0f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_infer_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/prism/prism/docker/src/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, infer_x, batch_size)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# threhshold\n",
    "f1_list = np.load(os.path.join(task_path, 'f1.npy'))\n",
    "thr_list = np.load(os.path.join(task_path, 'thr.npy'))\n",
    "thr = thr_list[np.argmax(f1_list)]\n",
    "\n",
    "y_key = data_loader.key\n",
    "y_pred = model.predict(data_loader.get_infer_data())\n",
    "y_pred = pd.DataFrame(y_pred, columns=['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/volume/local_test/model-10-0.341793.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.load(task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
