{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 두번째 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last expr -> all로 바꾸면 전체가 나온다. \"\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 250\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/user/anaconda3/envs/park/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Start\n",
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.004319429397583008\n",
      "data_loader extract_person time: 0.0033805370330810547\n",
      "data_loader extract_condition time: 0.002416849136352539\n",
      "data_loader extract_measurement time: 0.019445419311523438\n",
      "data_loader groupby_hour_condition time: 0.0058441162109375\n",
      "data_loader groupby_hour_measurement time: 1.705235242843628\n",
      "len of keylist:  674 shape of measurement:  (489, 302)\n",
      "data_loader make_person_sequence time: 0.03139615058898926\n",
      "X (674,)\n",
      "Y (674,)\n",
      "Key (674, 2)\n",
      "data_loader make_data time: 0.07201313972473145\n",
      "data_loader split_data time: 0.03528738021850586\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/user/anaconda3/envs/park/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = '../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "task_log_path = os.path.join(log_path, task_id)\n",
    "\n",
    "if not os.path.exists(task_path):\n",
    "  os.mkdir(task_path)\n",
    "if not os.path.exists(log_path):\n",
    "  os.mkdir(log_path)\n",
    "if not os.path.exists(task_log_path):\n",
    "  os.mkdir(task_log_path)\n",
    "\n",
    "print(\"Train Start\")\n",
    "\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path)\n",
    "model = SimpleRNNModel(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, GRU\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import layers\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input(shape=(None, data_loader.train_x.shape[2]))\n",
    "x = layers.Masking(mask_value=0)(model_input)\n",
    "x = layers.GRU(32, activation='tanh',\n",
    "               return_sequences=False)(model_input)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "\n",
    "model_output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "optimizer = adam(lr=0.001)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = task_path + \"/rnn.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 674 samples, validate on 674 samples\n",
      "Epoch 1/10\n",
      "674/674 [==============================] - 0s 386us/step - loss: 0.5427 - acc: 0.8858 - val_loss: 0.5244 - val_acc: 0.8858\n",
      "Epoch 2/10\n",
      "674/674 [==============================] - 0s 361us/step - loss: 0.5341 - acc: 0.8858 - val_loss: 0.5215 - val_acc: 0.8858\n",
      "Epoch 3/10\n",
      "674/674 [==============================] - 0s 344us/step - loss: 0.5401 - acc: 0.8843 - val_loss: 0.5187 - val_acc: 0.8858\n",
      "Epoch 4/10\n",
      "674/674 [==============================] - 0s 346us/step - loss: 0.5254 - acc: 0.8872 - val_loss: 0.5158 - val_acc: 0.8858\n",
      "Epoch 5/10\n",
      "674/674 [==============================] - 0s 354us/step - loss: 0.5245 - acc: 0.8843 - val_loss: 0.5128 - val_acc: 0.8858\n",
      "Epoch 6/10\n",
      "674/674 [==============================] - 0s 369us/step - loss: 0.5275 - acc: 0.8828 - val_loss: 0.5096 - val_acc: 0.8858\n",
      "Epoch 7/10\n",
      "674/674 [==============================] - 0s 342us/step - loss: 0.5272 - acc: 0.8858 - val_loss: 0.5063 - val_acc: 0.8858\n",
      "Epoch 8/10\n",
      "674/674 [==============================] - 0s 338us/step - loss: 0.5158 - acc: 0.8858 - val_loss: 0.5029 - val_acc: 0.8858\n",
      "Epoch 9/10\n",
      "674/674 [==============================] - 0s 349us/step - loss: 0.5115 - acc: 0.8843 - val_loss: 0.4993 - val_acc: 0.8858\n",
      "Epoch 10/10\n",
      "674/674 [==============================] - 0s 340us/step - loss: 0.5168 - acc: 0.8813 - val_loss: 0.4955 - val_acc: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5083ca590>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint(name, monitor='val_loss', verbose=0,\n",
    "                                         save_best_only=True, mode='min')]\n",
    "model.fit(data_loader.get_train_data()[0], data_loader.get_train_data()[1],\n",
    "            verbose=1,\n",
    "            epochs=10, batch_size=1000,\n",
    "          validation_data = data_loader.get_valid_data(),\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\tloss\tacc\tv_loss\tv_acc\n",
      "1\tnan\t0.9608\tnan\t0.9683\n",
      "2\tnan\t0.9714\tnan\t0.9683\n",
      "3\tnan\t0.9714\tnan\t0.9683\n",
      "4\tnan\t0.9714\tnan\t0.9683\n",
      "5\tnan\t0.9714\tnan\t0.9683\n",
      "6\tnan\t0.9714\tnan\t0.9683\n",
      "7\tnan\t0.9714\tnan\t0.9683\n",
      "8\tnan\t0.9714\tnan\t0.9683\n",
      "9\tnan\t0.9714\tnan\t0.9683\n",
      "10\tnan\t0.9714\tnan\t0.9683\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keys = hist.history.keys()\n",
    "print_keys = [key.replace('val_', 'v_') for key in (['epoch'] + list(keys))]\n",
    "print(\"\\t\".join(print_keys))\n",
    "for idx in range(len(hist.history['loss'])):\n",
    "  log_list = [str(idx+1)]\n",
    "  for key in keys:\n",
    "    log_list.append(\"%.4f\" % hist.history[key][idx])\n",
    "  \n",
    "  print(\"\\t\".join(log_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "from measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "\n",
    "condition_list = [\n",
    "  'A49.0',\n",
    "  'B34.9',\n",
    "  'B95.6',\n",
    "  'D18.0',\n",
    "  'D22.5',\n",
    "  'D22.9',\n",
    "  'D64.9',\n",
    "  'D75.8',\n",
    "  'E03.1',\n",
    "  'E03.8',\n",
    "  'E03.9',\n",
    "  'E22.2',\n",
    "  'E27.4',\n",
    "  'E55.0',\n",
    "  'E63.9',\n",
    "  'E80.7',\n",
    "  'E83.5',\n",
    "  'E87.1',\n",
    "  'E87.2',\n",
    "  'E88.9',\n",
    "  'F19.0',\n",
    "  'G00.2',\n",
    "  'G00.8',\n",
    "  'G25.3',\n",
    "  'G91.8',\n",
    "  'G93.8',\n",
    "  'H35.1',\n",
    "  'H65.9',\n",
    "  'H66.9',\n",
    "  'H90.2',\n",
    "  'H91.9',\n",
    "  'H93.2',\n",
    "  'I27.2',\n",
    "  'I28.8',\n",
    "  'I47.1',\n",
    "  'I50.9',\n",
    "  'J06.9',\n",
    "  'J12.3',\n",
    "  'J18.9',\n",
    "  'J21.1',\n",
    "  'J21.9',\n",
    "  'J40',\n",
    "  'J93.9',\n",
    "  'J98.1',\n",
    "  'K21.9',\n",
    "  'K40.3',\n",
    "  'K40.9',\n",
    "  'K56.5',\n",
    "  'K59.0',\n",
    "  'K61.0',\n",
    "  'K91.4',\n",
    "  'K92.2',\n",
    "  'L05.9',\n",
    "  'L22',\n",
    "  'L74.3',\n",
    "  'L92.8',\n",
    "  'M43.62',\n",
    "  'M67.4',\n",
    "  'N13.3',\n",
    "  'N17.9',\n",
    "  'N43.3',\n",
    "  'N48.1',\n",
    "  'N83.2',\n",
    "  'N94.8',\n",
    "  'O31.2',\n",
    "  'P00.0',\n",
    "  'P00.8',\n",
    "  'P00.9',\n",
    "  'P01.0',\n",
    "  'P01.1',\n",
    "  'P01.2',\n",
    "  'P01.3',\n",
    "  'P01.5',\n",
    "  'P01.7',\n",
    "  'P02.0',\n",
    "  'P02.1',\n",
    "  'P02.4',\n",
    "  'P02.7',\n",
    "  'P03.4',\n",
    "  'P04.0',\n",
    "  'P05.1',\n",
    "  'P05.9',\n",
    "  'P07.0',\n",
    "  'P07.1',\n",
    "  'P07.2',\n",
    "  'P07.3',\n",
    "  'P08.1',\n",
    "  'P22.0',\n",
    "  'P22.9',\n",
    "  'P25.1',\n",
    "  'P26.1',\n",
    "  'P26.9',\n",
    "  'P27.1',\n",
    "  'P28.2',\n",
    "  'P28.4',\n",
    "  'P29.3',\n",
    "  'P29.8',\n",
    "  'P35.1',\n",
    "  'P52.0',\n",
    "  'P52.1',\n",
    "  'P52.2',\n",
    "  'P52.3',\n",
    "  'P52.6',\n",
    "  'P52.8',\n",
    "  'P52.9',\n",
    "  'P54.0',\n",
    "  'P54.3',\n",
    "  'P56.9',\n",
    "  'P59.0',\n",
    "  'P59.8',\n",
    "  'P59.9',\n",
    "  'P61.0',\n",
    "  'P61.2',\n",
    "  'P70.0',\n",
    "  'P70.4',\n",
    "  'P72.2',\n",
    "  'P74',\n",
    "  'P76.0',\n",
    "  'P78.1',\n",
    "  'P81.9',\n",
    "  'P90',\n",
    "  'P91.2',\n",
    "  'P91.7',\n",
    "  'P92.9',\n",
    "  'P94.2',\n",
    "  'P96.8',\n",
    "  'Q04.3',\n",
    "  'Q04.6',\n",
    "  'Q04.8',\n",
    "  'Q10.3',\n",
    "  'Q10.5',\n",
    "  'Q21.1',\n",
    "  'Q25.0',\n",
    "  'Q27.0',\n",
    "  'Q31.5',\n",
    "  'Q33.6',\n",
    "  'Q42.3',\n",
    "  'Q53.1',\n",
    "  'Q53.2',\n",
    "  'Q53.9',\n",
    "  'Q62.0',\n",
    "  'Q63.2',\n",
    "  'Q64.4',\n",
    "  'Q65.2',\n",
    "  'Q66.8',\n",
    "  'Q67.3',\n",
    "  'Q69.1',\n",
    "  'Q82.8',\n",
    "  'Q87.2',\n",
    "  'Q89.9',\n",
    "  'R00.1',\n",
    "  'R04.8',\n",
    "  'R05',\n",
    "  'R09.2',\n",
    "  'R09.3',\n",
    "  'R09.8',\n",
    "  'R11',\n",
    "  'R16.2',\n",
    "  'R21',\n",
    "  'R25.1',\n",
    "  'R34',\n",
    "  'R49.0',\n",
    "  'R50.9',\n",
    "  'R56.8',\n",
    "  'R62.0',\n",
    "  'R62.8',\n",
    "  'R68.1',\n",
    "  'R73.9',\n",
    "  'R94.6',\n",
    "  'S09.9',\n",
    "  'S36.4',\n",
    "  'T18.9',\n",
    "  'T81.3',\n",
    "  'U83.0',\n",
    "  'Z00.0',\n",
    "  'Z01.0',\n",
    "  'Z13.9',\n",
    "  'Z26.9',\n",
    "  'Z38.0',\n",
    "  'Z38.1',\n",
    "  'Z38.3',\n",
    "  'Z38.6'\n",
    "]\n",
    "\n",
    "label_ratio = 0.1\n",
    "\n",
    "person_cols = ['PERSON_ID', 'BIRTH_DATETIME', 'GENDER_SOURCE_VALUE']\n",
    "condition_cols = ['PERSON_ID', 'CONDITION_START_DATETIME', 'CONDITION_SOURCE_VALUE']\n",
    "measurement_cols = ['PERSON_ID', 'MEASUREMENT_DATETIME', 'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "outcome_cols = ['SUBJECT_ID', 'COHORT_START_DATE', 'COHORT_END_DATE', 'LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_person(n=3):\n",
    "  data = []\n",
    "  gender = ['M', 'F']\n",
    "  for i in range(n):\n",
    "    data.append((i, randomtimes(n=1)[0], gender[i % 2]))\n",
    "  return data\n",
    "\n",
    "def generate_condition(n=5):\n",
    "  return random.choices(condition_list, k=n)\n",
    "\n",
    "def generate_measurement(n=5):\n",
    "  data = []\n",
    "  for k, stats in MEASUREMENT_SOURCE_VALUE_STATS.items():\n",
    "    values = np.random.normal(loc=stats['AVERAGE'], scale=stats['STANDARD DEVIATION'], size=n)\n",
    "    \n",
    "    data.extend([(k, v) for v in values])\n",
    "  np.random.shuffle(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_outcome_dts(stime, etime):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt).replace(minute=00, second=00) + datetime.timedelta(hours=1)\n",
    "  etime = datetime.datetime.strptime(etime, frmt) + datetime.timedelta(hours=1)\n",
    "  data = pd.date_range(start=stime, end=etime, freq='H')\n",
    "   \n",
    "  return data\n",
    "\n",
    "\n",
    "def randomtimes(stime='2019-01-01 00:00:00', etime='2019-02-01 00:00:00', n=10):\n",
    "  frmt = '%Y-%m-%d %H:%M:%S'\n",
    "  stime = datetime.datetime.strptime(stime, frmt)\n",
    "  if etime is None:\n",
    "    etime = stime + datetime.timedelta(days=2)\n",
    "  else:\n",
    "    etime = datetime.datetime.strptime(etime, frmt)\n",
    "  td = etime - stime\n",
    "  dts = [(random.random() * td + stime).strftime(frmt) for _ in range(n)]\n",
    "  dts.sort()\n",
    "  return dts\n",
    "\n",
    "\n",
    "def generate_dev_data(n=3, n_cond= 5, n_msmt=5):\n",
    "  \n",
    "  cond_list = []\n",
    "  msmt_list = []\n",
    "  person_list = generate_person(n)\n",
    "  outcome_list = []\n",
    "  for person in person_list:\n",
    "    # 환자를 생성\n",
    "    i = person[0]\n",
    "    min_date = person[1]\n",
    "    max_date = person[1]\n",
    "    # 진단을 생성\n",
    "    for dt, cond in zip(randomtimes(stime=min_date, n=n_cond), generate_condition(n_cond)):\n",
    "      cond_list.append((i, dt, cond))\n",
    "      if dt > max_date: # 가장 나중날짜를 코호트 종료날짜로\n",
    "        max_date = dt\n",
    "      # 바이탈사인 날짜 생성. 측정날짜는 진단 날짜부터\n",
    "      msmt_dts = randomtimes(stime=dt, n=n_msmt * len(MEASUREMENT_SOURCE_VALUE_STATS))\n",
    "      for dt2, msmt in zip(msmt_dts, generate_measurement(n_msmt)):\n",
    "        msmt_list.append((i, dt2, msmt[0], msmt[1]))\n",
    "        if dt2 > max_date:\n",
    "          max_date = dt2 # 가장 나중날짜를 코호트 종료날짜로\n",
    "    \n",
    "    # 코호트 종료날짜 1시간단위로 롤링\n",
    "    end_dts = generate_outcome_dts(min_date, max_date)\n",
    "    start_dts = [min_date] * len(end_dts)\n",
    "    subject_ids = [i] * len(end_dts)\n",
    "    # 언발란스로 라벨 생성\n",
    "    labels = np.random.choice([0, 1], size=len(end_dts), p=[1-label_ratio, label_ratio])\n",
    "    outcome_list.extend(list(zip(subject_ids, start_dts, end_dts, labels)))\n",
    "\n",
    "  # 샘플 데이터프레임 생성\n",
    "  person_df = pd.DataFrame(person_list, columns=person_cols)\n",
    "  condition_df = pd.DataFrame(cond_list, columns=condition_cols).sort_values(condition_cols[:2])\n",
    "  measurement_df = pd.DataFrame(msmt_list, columns=measurement_cols).sort_values(measurement_cols[:2])\n",
    "  outcome_df = pd.DataFrame(outcome_list, columns=outcome_cols)\n",
    "\n",
    "  # 샘플 데이터프레임 저장\n",
    "  data_path = os.path.join('../..', 'data')\n",
    "  dev_data = 'dev'\n",
    "  csv_files = {\n",
    "    'person': f'{dev_data}PERSON_NICU.csv',\n",
    "    'condition': f'{dev_data}CONDITION_OCCURRENCE_NICU.csv',\n",
    "    'measurement': f'{dev_data}MEASUREMENT_NICU.csv',\n",
    "    'outcome': f'{dev_data}OUTCOME_COHORT.csv'\n",
    "  }\n",
    "  person_df.to_csv(os.path.join(data_path, 'train', csv_files['person']), index=False)\n",
    "  condition_df.to_csv(os.path.join(data_path, 'train', csv_files['condition']), index=False)\n",
    "  measurement_df.to_csv(os.path.join(data_path, 'train', csv_files['measurement']), index=False)\n",
    "  outcome_df.to_csv(os.path.join(data_path, 'train', csv_files['outcome']), index=False)\n",
    "\n",
    "  print()\n",
    "  print('generated data shapes')\n",
    "  print(person_df.shape)\n",
    "  print(condition_df.shape)\n",
    "  print(measurement_df.shape)\n",
    "  print(outcome_df.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = os.environ.get('DEV_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_loader import DataLoader\n",
    "from model import SimpleRNNModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Start\n",
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.0055544376373291016\n",
      "data_loader extract_person time: 0.0038073062896728516\n",
      "data_loader extract_condition time: 0.0031697750091552734\n",
      "data_loader extract_measurement time: 4.865192413330078\n",
      "data_loader groupby_hour_condition time: 0.009240865707397461\n",
      "data_loader groupby_hour_measurement time: 34.73011374473572\n",
      "len of keylist:  2592 shape of measurement:  (5261, 16)\n",
      "data_loader make_person_sequence time: 0.07759714126586914\n",
      "X (2592,)\n",
      "Key (2592, 2)\n",
      "data_loader make_data time: 0.19432878494262695\n",
      "data_loader split_data time: 0.013419389724731445\n"
     ]
    }
   ],
   "source": [
    "data_path='../../data'\n",
    "\n",
    "task_id = os.environ.get('ID')\n",
    "if task_id is None:\n",
    "  task_id = 'local_test'\n",
    "\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "\n",
    "print(\"Inference Start\")\n",
    "\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'test'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                         is_train=False)\n",
    "model = SimpleRNNModel(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/volume/local_test'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.00478672981262207\n",
      "data_loader extract_person time: 0.0036013126373291016\n",
      "data_loader extract_condition time: 0.003130674362182617\n",
      "data_loader extract_measurement time: 4.7770044803619385\n",
      "data_loader groupby_hour_condition time: 0.006788492202758789\n",
      "data_loader groupby_hour_measurement time: 34.51263213157654\n",
      "len of keylist:  2592 shape of measurement:  (5261, 133)\n",
      "data_loader make_person_sequence time: 0.10256361961364746\n",
      "X (2592,)\n",
      "Y (2592,)\n",
      "Key (2592, 2)\n",
      "data_loader make_data time: 0.19785499572753906\n",
      "data_loader split_data time: 0.03417515754699707\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'test'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                         is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.train_x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1992, 128, 144)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load files {'person': 'PERSON_NICU.csv', 'condition': 'CONDITION_OCCURRENCE_NICU.csv', 'measurement': 'MEASUREMENT_NICU.csv', 'outcome': 'OUTCOME_COHORT.csv'}\n",
      "data_loader extract_outcome_cohort time: 0.0050868988037109375\n",
      "data_loader extract_person time: 0.004063844680786133\n",
      "data_loader extract_condition time: 0.0033578872680664062\n",
      "data_loader extract_measurement time: 4.715225696563721\n",
      "data_loader groupby_hour_condition time: 0.007773876190185547\n",
      "data_loader groupby_hour_measurement time: 34.61149883270264\n",
      "len of keylist:  2592 shape of measurement:  (5261, 133)\n",
      "data_loader make_person_sequence time: 0.10039520263671875\n",
      "X (2592,)\n",
      "Key (2592, 2)\n",
      "data_loader make_data time: 0.2467348575592041\n",
      "data_loader split_data time: 0.03818035125732422\n"
     ]
    }
   ],
   "source": [
    "data_loader2 = DataLoader(data_path=os.path.join(data_path, 'test'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                         is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2592, 128, 144)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader2.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
