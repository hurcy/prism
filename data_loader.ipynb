{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from docker.src.measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "from datetime import datetime, timedelta, time as datetime_time, timezone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "VALUE_MAP = ['HR','RR','SpO2','Pulse','Temp','ABPm','ABPd','ABPs','NBPm','NBPs','NBPd','SPO2-%','SPO2-R',\n",
    "'Resp','PVC','ST-II','etCO2','SpO2 r','imCO2','ST-V1','ST-I','ST-III','ST-aVF','ST-aVL','ST-aVR',\n",
    "'awRR','CVPm','AoM','ST-V2','ST-V3','ST-V4','ST-V5','ST-V6','SpO2T','T1','TV','Cdyn','PEEP','RRaw',\n",
    "'TVin','inO2','AoD','AoS','InsTi','MINVOL','MnAwP','PIP','MVin','PB','Poccl','Pplat',\n",
    "'MV','Patm','Ppeak','Rinsp','ST-V','sInsTi','sPEEP','sTV','sTrig','sPSV','Rexp','highP',\n",
    "'sAPkFl','sAWRR','sFIO2','sPIF','sMV','sO2','sRisTi','ARTd','ARTm','ARTs','PAPm','sSIMV']\n",
    "\n",
    "MEASUREMENT_NORMALIZATION = ['mean', 'predefined']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/train'\n",
    "common_path='./data/volume'\n",
    "\n",
    "task_path='./data/volume/local_test'\n",
    "# os.mkdir(task_path)\n",
    "is_train=True\n",
    "\n",
    "group_hour=1\n",
    "timestep_per_data=128\n",
    "\n",
    "measurement_normalize='mean'\n",
    "\n",
    "condition_min_limit=0\n",
    "condition_group=False\n",
    "\n",
    "valid_size=0.2\n",
    "data_split_random_seed=1235\n",
    "pytest=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.011013984680175781\n",
      "data_loader extract_person time: 0.0070133209228515625\n",
      "data_loader extract_condition time: 0.006035566329956055\n",
      "data_loader extract_measurement time: 2.281010866165161\n"
     ]
    }
   ],
   "source": [
    "  def extract_outcome_cohort():\n",
    "    start_time = time.time()\n",
    "    cohort_df = pd.read_csv(os.path.join(data_path, 'OUTCOME_COHORT.csv'), encoding='windows-1252')\n",
    "\n",
    "    cohort_df.COHORT_START_DATE = pd.to_datetime(cohort_df.COHORT_START_DATE)\n",
    "    cohort_df.COHORT_END_DATE = pd.to_datetime(cohort_df.COHORT_END_DATE)\n",
    "    print(\"data_loader extract_outcome_cohort time:\", time.time() - start_time)\n",
    "    return cohort_df\n",
    "\n",
    "  def extract_person():\n",
    "    start_time = time.time()\n",
    "    person_df = pd.read_csv(os.path.join(data_path, 'PERSON_NICU.csv'), encoding='windows-1252')\n",
    "    person_df = pd.concat([\n",
    "        person_df[['PERSON_ID', 'BIRTH_DATETIME']],\n",
    "        pd.get_dummies(person_df.GENDER_SOURCE_VALUE, prefix='gender')\n",
    "    ], axis=1)\n",
    "\n",
    "    # 생일 컬럼 타입 설정\n",
    "    person_df.BIRTH_DATETIME = pd.to_datetime(person_df.BIRTH_DATETIME, utc=True)\n",
    "    # 여성/남성 컬럼 1개로 처리\n",
    "    person_df.rename(columns={'gender_M': 'GENDER'}, inplace=True)\n",
    "    if 'gender_F' in person_df.columns:\n",
    "      del person_df['gender_F']\n",
    "\n",
    "    print(\"data_loader extract_person time:\", time.time() - start_time)\n",
    "    return person_df\n",
    "\n",
    "  def extract_condition():\n",
    "    start_time = time.time()\n",
    "    condition_df = pd.read_csv(os.path.join(data_path, 'CONDITION_OCCURRENCE_NICU.csv'), encoding='windows-1252',\n",
    "                               usecols=['PERSON_ID', 'CONDITION_SOURCE_VALUE', 'CONDITION_START_DATETIME'])\n",
    "    # Null 이거나 값이 빈 것을 날림\n",
    "    condition_df = condition_df[pd.notnull(condition_df.CONDITION_SOURCE_VALUE)]\n",
    "    condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.str.len() > 0]\n",
    "\n",
    "    if condition_group:\n",
    "      condition_df.CONDITION_SOURCE_VALUE = condition_df.CONDITION_SOURCE_VALUE.str.slice(stop=3)\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    condition_df.CONDITION_START_DATETIME = pd.to_datetime(condition_df.CONDITION_START_DATETIME, utc=True)\n",
    "\n",
    "    print(\"data_loader extract_condition time:\", time.time() - start_time)\n",
    "    return condition_df\n",
    "\n",
    "  def extract_measurement():\n",
    "    start_time = time.time()\n",
    "    measurement_df = pd.read_csv(os.path.join(data_path, 'MEASUREMENT_NICU.csv'), \n",
    "                                 encoding='windows-1252',\n",
    "                                 usecols=['PERSON_ID', 'MEASUREMENT_DATETIME',\n",
    "                                          'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "                                 )\n",
    "#     if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "#       # source_value 맵핑\n",
    "#       source_value_invert_map = {}\n",
    "#       for new_value in MEASUREMENT_SOURCE_VALUE_MAP:\n",
    "#         for table_value in MEASUREMENT_SOURCE_VALUE_MAP[new_value]:\n",
    "#           source_value_invert_map[table_value] = new_value\n",
    "#       measurement_df.MEASUREMENT_SOURCE_VALUE = measurement_df.MEASUREMENT_SOURCE_VALUE.replace(source_value_invert_map)\n",
    "\n",
    "      # 맵핑이된 정보만 남긴다\n",
    "    measurement_df = measurement_df[measurement_df.MEASUREMENT_SOURCE_VALUE.isin(VALUE_MAP)]\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    measurement_df.MEASUREMENT_DATETIME = pd.to_datetime(measurement_df.MEASUREMENT_DATETIME, utc=True)\n",
    "\n",
    "    # source_value별 평균값 추출\n",
    "    if is_train:\n",
    "      measurement_mean_df = measurement_df.groupby('MEASUREMENT_SOURCE_VALUE').VALUE_AS_NUMBER.mean()\n",
    "      measurement_mean_df.to_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "    else:\n",
    "      # inference일 경우 저장된 걸 불러온다\n",
    "      measurement_mean_df = pd.read_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "\n",
    "    print(\"data_loader extract_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df, measurement_mean_df\n",
    "cohort_df = extract_outcome_cohort()\n",
    "person_df = extract_person()\n",
    "condition_df = extract_condition()\n",
    "measurement_df,measurement_mean_df = extract_measurement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def groupby_hour_condition( condition_df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    condition_df['CONDITION_DATE'] = condition_df.CONDITION_START_DATETIME.dt.date\n",
    "    condition_df['CONDITION_DATE'] = pd.to_datetime(condition_df.CONDITION_DATE, utc=True)\n",
    "\n",
    "    if is_train and condition_min_limit > 0:\n",
    "      condition_group = condition_df.groupby('CONDITION_SOURCE_VALUE').PERSON_ID.count()\n",
    "      condition_group = condition_group[condition_group > condition_min_limit].index\n",
    "\n",
    "      condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.isin(condition_group)]\n",
    "\n",
    "    # 진단은 시간이 없다. 당일의 마지막에 진단 받은걸로 가정한다\n",
    "    condition_df['HOURGRP'] = 23 // group_hour\n",
    "\n",
    "    group_cols = ['PERSON_ID', 'CONDITION_DATE', 'HOURGRP', 'CONDITION_SOURCE_VALUE']\n",
    "\n",
    "    condition_df['DUMMY'] = condition_df['CONDITION_SOURCE_VALUE']\n",
    "    condition_df = condition_df.groupby(group_cols) \\\n",
    "        .DUMMY.count().unstack().reset_index().fillna(0)\n",
    "\n",
    "    condition_df = condition_df.rename(columns={'CONDITION_DATE': 'DATE'})\n",
    "\n",
    "    condition_col_filename = os.path.join(task_path, 'condition_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(condition_col_filename, np.array(condition_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      condition_cols = np.load(condition_col_filename, allow_pickle=True)\n",
    "      new_condition_list = []\n",
    "      for col in condition_cols:\n",
    "        if col in condition_df.columns:\n",
    "          new_condition_list.append(condition_df[col])\n",
    "        else:\n",
    "          new_condition_list.append(pd.Series([0] * condition_df.shape[0]))\n",
    "\n",
    "      condition_df = pd.concat(new_condition_list, axis=1)\n",
    "      condition_df.columns = condition_cols\n",
    "    print(\"data_loader groupby_hour_condition time:\", time.time() - start_time)\n",
    "    return condition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader groupby_hour_condition time: 0.01300191879272461\n"
     ]
    }
   ],
   "source": [
    "condition_df = groupby_hour_condition(condition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _clip_measurement(measurement_source_value, value_as_number):\n",
    "    if value_as_number > MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']\n",
    "    elif value_as_number < MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']\n",
    "    return value_as_number\n",
    "\n",
    "  def groupby_hour_measurement(measurement_df):\n",
    "    start_time = time.time()\n",
    "    # timestamp로 join 하기 위하여 시간 포맷을 utc로 통일\n",
    "    measurement_df['MEASUREMENT_DATE'] = measurement_df.MEASUREMENT_DATETIME.dt.date\n",
    "    measurement_df['MEASUREMENT_DATE'] = pd.to_datetime(measurement_df.MEASUREMENT_DATE, utc=True)\n",
    "\n",
    "    measurement_df['MEASUREMENT_HOUR'] = measurement_df.MEASUREMENT_DATETIME.dt.hour\n",
    "    measurement_df['MEASUREMENT_HOURGRP'] = measurement_df.MEASUREMENT_HOUR // group_hour\n",
    "\n",
    "    # 평균값 이용하여 Normalize\n",
    "    if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "      measurement_df = pd.merge(measurement_df,\n",
    "                                measurement_mean_df.reset_index().rename(\n",
    "                                    columns={'VALUE_AS_NUMBER': 'MEAN_VALUE'}),\n",
    "                                on='MEASUREMENT_SOURCE_VALUE', how='left')\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.VALUE_AS_NUMBER / measurement_df.MEAN_VALUE\n",
    "    # 생체신호 범위를 이용하여 Normalize\n",
    "    elif measurement_normalize == MEASUREMENT_NORMALIZATION[1]:\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.apply(lambda row:\n",
    "                                                            _clip_measurement(\n",
    "                                                                row['MEASUREMENT_SOURCE_VALUE'],\n",
    "                                                                row['VALUE_AS_NUMBER']),\n",
    "                                                            axis=1)\n",
    "\n",
    "      # TODO\n",
    "    group_cols = ['PERSON_ID', 'MEASUREMENT_DATE', 'MEASUREMENT_HOURGRP', 'MEASUREMENT_SOURCE_VALUE']\n",
    "    agg_list = ['count', 'min', 'max', 'mean', 'std', 'var']\n",
    "    measurement_df['VALUE_DIFF'] = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.diff()\n",
    "\n",
    "    measurement_diff_df = pd.pivot_table(measurement_df, \n",
    "                                         values='VALUE_DIFF', index=group_cols[:-1],\n",
    "                                         columns='MEASUREMENT_SOURCE_VALUE', aggfunc=['mean','max','min'])\n",
    "\n",
    "    measurement_diff_df.columns = [('diff', '{}_{}'.format(v[0],v[1])) for v in measurement_diff_df.columns]\n",
    "\n",
    "    measurement_df = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.agg(agg_list).fillna(0).unstack().fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    measurement_df = pd.concat([measurement_df, measurement_diff_df], axis=1).reset_index()\n",
    "\n",
    "    if measurement_df.isnull().sum().sum() >0:\n",
    "        print(\"there is Na after interpolation\")\n",
    "        measurement_df = measurement_df.fillna(0)\n",
    "        \n",
    "    # 사용한 후 삭제\n",
    "    del measurement_diff_df\n",
    "    # 컬럼 이름 정제 (그룹화 하기 쉽게)\n",
    "    new_cols = []\n",
    "    for col in measurement_df.columns:\n",
    "      \n",
    "      if col[1] == '':\n",
    "        new_cols.append(col[0])\n",
    "      elif col[0] in agg_list + ['diff']:\n",
    "        new_cols.append((col[1], col[0]))\n",
    "    measurement_df.columns = new_cols\n",
    "\n",
    "#     #minmax scale\n",
    "#     scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "#     scaler = scaler.fit(measurement_df.iloc[:,3:])\n",
    "#     measurement_df.iloc[:,3:] = scaler.transform(measurement_df.iloc[:,3:])\n",
    "    \n",
    "    measurement_df = measurement_df.rename(columns={'MEASUREMENT_DATE': 'DATE',\n",
    "                                                    'MEASUREMENT_HOURGRP': 'HOURGRP'})\n",
    "\n",
    "    measurement_col_filename = os.path.join(task_path, 'measurement_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(measurement_col_filename, np.array(measurement_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      measurement_cols = np.load(measurement_col_filename, allow_pickle=True)\n",
    "      new_measurement_list = []\n",
    "      for col in measurement_cols:\n",
    "        if col in measurement_df.columns:\n",
    "          new_measurement_list.append(measurement_df[col])\n",
    "        else:\n",
    "          new_measurement_list.append(pd.Series([0] * measurement_df.shape[0]))\n",
    "\n",
    "      measurement_df = pd.concat(new_measurement_list, axis=1)\n",
    "      measurement_df.columns = measurement_cols\n",
    "    print(\"data_loader groupby_hour_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 7.197989463806152\n"
     ]
    }
   ],
   "source": [
    "measurement_df = groupby_hour_measurement(measurement_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder 작업중 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.009999513626098633\n",
      "data_loader extract_person time: 0.006041765213012695\n",
      "data_loader extract_condition time: 0.00601649284362793\n",
      "data_loader extract_measurement time: 2.2839856147766113\n",
      "data_loader groupby_hour_condition time: 0.0169980525970459\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 6.957036018371582\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               7020      \n",
      "=================================================================\n",
      "Total params: 29,228\n",
      "Trainable params: 29,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "data_loader autoencoder.predict time: 2.035015106201172\n",
      "data_loader make_person_sequence time: 0.07996082305908203\n",
      "X (1592,)\n",
      "Y (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.19301915168762207\n",
      "on_split\n",
      "data_loader split_data time: 0.03295612335205078\n"
     ]
    }
   ],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "import os\n",
    "task_path='./data/volume/local_test'\n",
    "data_loader = DataLoader(data_path=os.path.join('./data', 'train'),\n",
    "                         common_path=os.path.join(\"./data\", 'volume'),\n",
    "                         measurement_normalize='mean',\n",
    "                         is_train = True,\n",
    "                         task_path=task_path,\n",
    "                        switch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               7020      \n",
      "=================================================================\n",
      "Total params: 29,228\n",
      "Trainable params: 29,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 726.2963 - val_loss: 717.0902\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 717.09021, saving model to ./data/volume/local_test\\encoder-01-717.090210.hdf5\n",
      "Epoch 2/10\n",
      " - 0s - loss: 721.8182 - val_loss: 714.8348\n",
      "\n",
      "Epoch 00002: val_loss improved from 717.09021 to 714.83484, saving model to ./data/volume/local_test\\encoder-02-714.834839.hdf5\n",
      "Epoch 3/10\n",
      " - 0s - loss: 719.5919 - val_loss: 712.9244\n",
      "\n",
      "Epoch 00003: val_loss improved from 714.83484 to 712.92444, saving model to ./data/volume/local_test\\encoder-03-712.924438.hdf5\n",
      "Epoch 4/10\n",
      " - 0s - loss: 717.7097 - val_loss: 711.1398\n",
      "\n",
      "Epoch 00004: val_loss improved from 712.92444 to 711.13983, saving model to ./data/volume/local_test\\encoder-04-711.139832.hdf5\n",
      "Epoch 5/10\n",
      " - 0s - loss: 715.9441 - val_loss: 710.1699\n",
      "\n",
      "Epoch 00005: val_loss improved from 711.13983 to 710.16992, saving model to ./data/volume/local_test\\encoder-05-710.169922.hdf5\n",
      "Epoch 6/10\n",
      " - 0s - loss: 714.9875 - val_loss: 709.7444\n",
      "\n",
      "Epoch 00006: val_loss improved from 710.16992 to 709.74445, saving model to ./data/volume/local_test\\encoder-06-709.744446.hdf5\n",
      "Epoch 7/10\n",
      " - 0s - loss: 714.5642 - val_loss: 709.5330\n",
      "\n",
      "Epoch 00007: val_loss improved from 709.74445 to 709.53302, saving model to ./data/volume/local_test\\encoder-07-709.533020.hdf5\n",
      "Epoch 8/10\n",
      " - 0s - loss: 714.3526 - val_loss: 709.4155\n",
      "\n",
      "Epoch 00008: val_loss improved from 709.53302 to 709.41553, saving model to ./data/volume/local_test\\encoder-08-709.415527.hdf5\n",
      "Epoch 9/10\n",
      " - 0s - loss: 714.2343 - val_loss: 709.3340\n",
      "\n",
      "Epoch 00009: val_loss improved from 709.41553 to 709.33405, saving model to ./data/volume/local_test\\encoder-09-709.334045.hdf5\n",
      "Epoch 10/10\n",
      " - 0s - loss: 714.1519 - val_loss: 709.2733\n",
      "\n",
      "Epoch 00010: val_loss improved from 709.33405 to 709.27325, saving model to ./data/volume/local_test\\encoder-10-709.273254.hdf5\n"
     ]
    }
   ],
   "source": [
    "from docker.src.model import Autoencoder\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "\n",
    "callbacks = [\n",
    "ModelCheckpoint(filepath=os.path.join(task_path, 'encoder-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=True\n",
    "),\n",
    "EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame(autoen.predict(measurement_df.iloc[:,3:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "measurement_as = pd.concat([measurement_df.iloc[:,:3], output],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>DATE</th>\n",
       "      <th>HOURGRP</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2036-04-22 00:00:00+00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463015</td>\n",
       "      <td>3.018350</td>\n",
       "      <td>3.404919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175673</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.442196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.768957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.561999</td>\n",
       "      <td>2.057318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2036-04-23 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.724736</td>\n",
       "      <td>2.757065</td>\n",
       "      <td>2.549405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.289711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.725230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.966726</td>\n",
       "      <td>2.557830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2036-04-23 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.764820</td>\n",
       "      <td>2.749552</td>\n",
       "      <td>2.584313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.730961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.641720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922184</td>\n",
       "      <td>2.560203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2036-04-23 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.772781</td>\n",
       "      <td>2.736174</td>\n",
       "      <td>2.570029</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.730395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.916005</td>\n",
       "      <td>2.567777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>2036-04-23 00:00:00+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.793007</td>\n",
       "      <td>2.718867</td>\n",
       "      <td>2.597193</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.739558</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.653624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.879824</td>\n",
       "      <td>2.573025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2276</th>\n",
       "      <td>4.771510e+16</td>\n",
       "      <td>2036-05-05 00:00:00+00:00</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.675696</td>\n",
       "      <td>3.037452</td>\n",
       "      <td>2.716092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.406900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.138141</td>\n",
       "      <td>2.400742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2277</th>\n",
       "      <td>4.771510e+16</td>\n",
       "      <td>2036-05-05 00:00:00+00:00</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.527802</td>\n",
       "      <td>3.248977</td>\n",
       "      <td>2.450779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.406554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.237040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.379658</td>\n",
       "      <td>2.302514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2278</th>\n",
       "      <td>4.771510e+16</td>\n",
       "      <td>2036-05-06 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.784357</td>\n",
       "      <td>3.198696</td>\n",
       "      <td>2.215753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.351112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.422162</td>\n",
       "      <td>2.291072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2279</th>\n",
       "      <td>4.771510e+16</td>\n",
       "      <td>2036-05-06 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.017043</td>\n",
       "      <td>3.245299</td>\n",
       "      <td>2.080878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.467436</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.255916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.631893</td>\n",
       "      <td>2.546199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2280</th>\n",
       "      <td>4.771510e+16</td>\n",
       "      <td>2036-05-06 00:00:00+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.072558</td>\n",
       "      <td>2.052509</td>\n",
       "      <td>2.317386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.009669</td>\n",
       "      <td>0.282522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.926234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600549</td>\n",
       "      <td>1.035794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318305</td>\n",
       "      <td>3.361727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2281 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         PERSON_ID                      DATE  HOURGRP    0         1  \\\n",
       "0     1.000000e+01 2036-04-22 00:00:00+00:00       23  0.0  1.463015   \n",
       "1     1.000000e+01 2036-04-23 00:00:00+00:00        0  0.0  1.724736   \n",
       "2     1.000000e+01 2036-04-23 00:00:00+00:00        1  0.0  1.764820   \n",
       "3     1.000000e+01 2036-04-23 00:00:00+00:00        2  0.0  1.772781   \n",
       "4     1.000000e+01 2036-04-23 00:00:00+00:00        3  0.0  1.793007   \n",
       "...            ...                       ...      ...  ...       ...   \n",
       "2276  4.771510e+16 2036-05-05 00:00:00+00:00       22  0.0  1.675696   \n",
       "2277  4.771510e+16 2036-05-05 00:00:00+00:00       23  0.0  1.527802   \n",
       "2278  4.771510e+16 2036-05-06 00:00:00+00:00        0  0.0  1.784357   \n",
       "2279  4.771510e+16 2036-05-06 00:00:00+00:00        1  0.0  2.017043   \n",
       "2280  4.771510e+16 2036-05-06 00:00:00+00:00        2  0.0  1.072558   \n",
       "\n",
       "             2         3    4         5         6  ...   54   55        56  \\\n",
       "0     3.018350  3.404919  0.0  0.000000  0.175673  ...  0.0  0.0  4.442196   \n",
       "1     2.757065  2.549405  0.0  0.000000  0.289711  ...  0.0  0.0  4.725230   \n",
       "2     2.749552  2.584313  0.0  0.000000  0.364534  ...  0.0  0.0  4.730961   \n",
       "3     2.736174  2.570029  0.0  0.000000  0.371790  ...  0.0  0.0  4.730395   \n",
       "4     2.718867  2.597193  0.0  0.000000  0.411221  ...  0.0  0.0  4.739558   \n",
       "...        ...       ...  ...       ...       ...  ...  ...  ...       ...   \n",
       "2276  3.037452  2.716092  0.0  0.000000  0.014164  ...  0.0  0.0  4.406900   \n",
       "2277  3.248977  2.450779  0.0  0.000000  0.000000  ...  0.0  0.0  4.406554   \n",
       "2278  3.198696  2.215753  0.0  0.000000  0.413760  ...  0.0  0.0  3.351112   \n",
       "2279  3.245299  2.080878  0.0  0.000000  0.500519  ...  0.0  0.0  3.467436   \n",
       "2280  2.052509  2.317386  0.0  1.009669  0.282522  ...  0.0  0.0  2.926234   \n",
       "\n",
       "       57   58        59        60   61        62        63  \n",
       "0     0.0  0.0  0.000000  0.768957  0.0  0.561999  2.057318  \n",
       "1     0.0  0.0  0.000000  0.630201  0.0  0.966726  2.557830  \n",
       "2     0.0  0.0  0.000000  0.641720  0.0  0.922184  2.560203  \n",
       "3     0.0  0.0  0.000000  0.642221  0.0  0.916005  2.567777  \n",
       "4     0.0  0.0  0.000000  0.653624  0.0  0.879824  2.573025  \n",
       "...   ...  ...       ...       ...  ...       ...       ...  \n",
       "2276  0.0  0.0  0.000000  0.239990  0.0  1.138141  2.400742  \n",
       "2277  0.0  0.0  0.000000  0.237040  0.0  1.379658  2.302514  \n",
       "2278  0.0  0.0  0.000000  0.055041  0.0  1.422162  2.291072  \n",
       "2279  0.0  0.0  0.000000  0.255916  0.0  1.631893  2.546199  \n",
       "2280  0.0  0.0  0.600549  1.035794  0.0  0.318305  3.361727  \n",
       "\n",
       "[2281 rows x 67 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measurement_as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoen.model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x124552c0c88>,\n",
       " <keras.layers.core.Dense at 0x124552c0fc8>,\n",
       " <keras.layers.core.Dense at 0x124552d9c48>,\n",
       " <keras.layers.core.Dense at 0x12455358b88>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoen.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-778e9d08c7a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import model_from_json \n",
    "\n",
    "\n",
    "model_path = tf.train.latest_checkpoint(task_path)\n",
    "if model_path is None:\n",
    "  file_name = sorted([file_name for file_name in os.listdir(task_path) if file_name.endswith('.hdf5') and file_name.startswith('encoder')])[-1]\n",
    "  model_path = os.path.join(task_path, file_name)\n",
    "\n",
    "model = model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder-10-725.665955.hdf5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "input_img = Input(shape=(train_measure.iloc[:,3:].shape[1],))\n",
    "layer1=autoen.model.layers[1]\n",
    "layer2=autoen.model.layers[2]\n",
    "\n",
    "encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "output=encoder.predict(train_measure.iloc[:,3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.9231129 , ..., 2.1713006 , 0.4146613 ,\n",
       "        2.4669633 ],\n",
       "       [0.        , 0.        , 1.7895968 , ..., 2.17478   , 0.41060454,\n",
       "        2.4875143 ],\n",
       "       [0.        , 0.        , 1.824985  , ..., 2.1258125 , 0.11797369,\n",
       "        2.0025125 ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.8242193 , ..., 2.1266065 , 0.11741423,\n",
       "        2.0028756 ],\n",
       "       [0.        , 0.        , 1.8244132 , ..., 2.126528  , 0.11736455,\n",
       "        2.0026877 ],\n",
       "       [0.        , 0.        , 1.8885305 , ..., 2.1614344 , 0.43662977,\n",
       "        2.492919  ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_img = Input(shape=(self.train_measure.shape[1],))\n",
    "    layer1=self.model.layers[1]\n",
    "    layer2=self.model.layers[2]\n",
    "\n",
    "    encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "    output=encoder.predict(total_measure)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                          train_size=(1 - valid_size),\n",
    "                                                          test_size=valid_size,\n",
    "                                                          random_state=data_split_random_seed)\n",
    "    autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "    autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "    self.model = autoen\n",
    "    output = autoen.predict(measurement_df.iloc[:,3:])\n",
    "    mesurement_df[:,3:] = output\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "\n",
    "output = autoen.predict(measurement_df.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def make_person_sequence():\n",
    "    start_time = time.time()\n",
    "    # 환자별로 데이터의 시작시간과 종료시간을 구한다.\n",
    "    timerange_df = cohort_df.groupby('SUBJECT_ID').agg({'COHORT_START_DATE': 'min', 'COHORT_END_DATE': 'max'})\n",
    "    timerange_df['START_DATE'] = timerange_df.COHORT_START_DATE.dt.date\n",
    "    timerange_df['START_HOURGRP'] = timerange_df.COHORT_START_DATE.dt.hour // group_hour\n",
    "    timerange_df['END_DATE'] = timerange_df.COHORT_END_DATE.dt.date\n",
    "    timerange_df['END_HOURGRP'] = timerange_df.COHORT_END_DATE.dt.hour // group_hour\n",
    "    timerange_df = timerange_df.drop(['COHORT_START_DATE', 'COHORT_END_DATE'], axis=1)\n",
    "\n",
    "    demographic_ary = person_df.sort_values('PERSON_ID', ascending=True).values\n",
    "    condition_ary = condition_df.sort_values(['PERSON_ID', 'DATE', 'HOURGRP'], ascending=True).values\n",
    "    measurement_ary = measurement_df.sort_values(['PERSON_ID', 'DATE', 'HOURGRP'], ascending=True).values\n",
    "    timerange_ary = timerange_df.sort_values('SUBJECT_ID', ascending=True).reset_index().values\n",
    "\n",
    "    demographic_cols = [\"AGE_HOUR\", \"GENDER\"]\n",
    "    condition_cols = condition_df.columns[3:]\n",
    "    measurement_cols = measurement_df.columns[3:]\n",
    "\n",
    "    # 빈 Time Range 없게 시간대 정보를 채움\n",
    "    max_hourgrp = (24 // group_hour) - 1\n",
    "\n",
    "    key_list = []\n",
    "    for person_id, start_date, start_hourgrp, end_date, end_hourgrp in timerange_ary:\n",
    "      cur_date = start_date\n",
    "      cur_hourgrp = start_hourgrp\n",
    "\n",
    "      while True:\n",
    "        key_list.append((person_id, cur_date, cur_hourgrp))\n",
    "\n",
    "        cur_hourgrp += 1                  # 1 그룹시간만큼 탐색\n",
    "        if cur_hourgrp > max_hourgrp:     # 다음 날짜로 넘어감\n",
    "          cur_date = cur_date + timedelta(days=1)\n",
    "          cur_hourgrp = 0\n",
    "\n",
    "        if cur_date > end_date or \\\n",
    "           (cur_date == end_date and cur_hourgrp >= end_hourgrp):\n",
    "          # 끝까지 탐색함\n",
    "          break\n",
    "\n",
    "    # 시간대 정보에 따라 데이터를 채워 넣는다\n",
    "    demographic_idx = condition_idx = measurement_idx = 0\n",
    "    prev_person_id = None\n",
    "    prev_conditions = None\n",
    "\n",
    "    data_cols = list(demographic_cols) + list(measurement_cols) + list(condition_cols)\n",
    "    data_list = np.zeros((len(key_list), len(data_cols)), dtype=np.float32)\n",
    "    for idx, row in enumerate(key_list):\n",
    "      person_id, date, hourgrp = row\n",
    "\n",
    "      col_start_idx = col_end_idx = 0\n",
    "      col_end_idx += len(demographic_cols)\n",
    "      # Demographic 추가\n",
    "      while True:\n",
    "        if demographic_idx >= len(demographic_ary):\n",
    "          break\n",
    "\n",
    "        demographic_row = demographic_ary[demographic_idx]\n",
    "        demographic_person_id = demographic_row[0]\n",
    "        # 시간 계산을 위해 tz를 동일하게 맞춤.\n",
    "        demographic_age = datetime.combine(date, datetime_time(hour=hourgrp, tzinfo=timezone.utc)).astimezone(\n",
    "            pytz.utc) - demographic_row[1]\n",
    "        demographic_gender = demographic_row[2]\n",
    "        demographic_data = [demographic_age.total_seconds() // 3600., demographic_gender]\n",
    "\n",
    "        state = 0       # 0: 다음 데이터 탐색 1: 맞는 데이터 찾음 2: 맞는 데이터 없음\n",
    "        if demographic_person_id > person_id:       # 다음 환자로 넘어감\n",
    "          state = 2\n",
    "        elif demographic_person_id == person_id:  # 맞는 데이터\n",
    "          state = 1\n",
    "\n",
    "        if state == 0:                  # 계속 탐색\n",
    "          demographic_idx += 1\n",
    "        elif state == 1:                # 데이터 찾음\n",
    "          data_list[idx, col_start_idx:col_end_idx] = demographic_data\n",
    "          break\n",
    "        elif state == 2:                # 맞는 데이터가 없음\n",
    "          break\n",
    "\n",
    "      # Measurement 탐색\n",
    "      col_start_idx = col_end_idx\n",
    "      col_end_idx += len(measurement_cols)\n",
    "      while True:\n",
    "        if measurement_idx >= len(measurement_ary):\n",
    "          break\n",
    "\n",
    "        measurement_row = measurement_ary[measurement_idx]\n",
    "        measurement_person_id = measurement_row[0]\n",
    "        measurement_date = measurement_row[1]\n",
    "        measurement_hourgrp = measurement_row[2]\n",
    "        measurement_data = measurement_row[3:]\n",
    "\n",
    "        state = 0       # 0: 다음 데이터 탐색 1: 맞는 데이터 찾음 2: 맞는 데이터 없음\n",
    "        if measurement_person_id > person_id:       # 다음 환자로 넘어감\n",
    "          state = 2\n",
    "        elif measurement_person_id == person_id:\n",
    "          if measurement_date.date() > date:               # 다음 날짜로 넘어감\n",
    "            state = 2\n",
    "          elif measurement_date.date() == date:\n",
    "            if measurement_hourgrp > hourgrp:       # 다음 그룹시간으로 넘어감\n",
    "              state = 2\n",
    "            elif measurement_hourgrp == hourgrp:    # 맞는 데이터\n",
    "              state = 1\n",
    "\n",
    "        if state == 0:                  # 계속 탐색\n",
    "          measurement_idx += 1\n",
    "        elif state == 1:                # 데이터 찾음\n",
    "          data_list[idx, col_start_idx:col_end_idx] = measurement_data\n",
    "          measurement_idx += 1\n",
    "          break\n",
    "        elif state == 2:                # 맞는 데이터가 없음\n",
    "          break\n",
    "\n",
    "      # Condition 탐색\n",
    "      col_start_idx = col_end_idx\n",
    "      col_end_idx += len(condition_cols)\n",
    "      # 이전과 다른 환자임. condition정보 reset\n",
    "      if prev_person_id != person_id:\n",
    "        prev_conditions = np.array([0] * len(condition_cols))\n",
    "\n",
    "      while True:\n",
    "        if condition_idx >= len(condition_ary):\n",
    "          break\n",
    "\n",
    "        condition_row = condition_ary[condition_idx]\n",
    "        condition_person_id = condition_row[0]\n",
    "        condition_date = condition_row[1]\n",
    "        condition_hourgrp = condition_row[2]\n",
    "        condition_data = condition_row[3:]\n",
    "\n",
    "        state = 0       # 0: 다음 데이터 탐색 1: 맞는 데이터 찾음 2: 맞는 데이터 없음\n",
    "        if condition_person_id > person_id:       # 다음 환자로 넘어감\n",
    "          state = 2\n",
    "        elif condition_person_id == person_id:\n",
    "          if condition_date.date() > date:               # 다음 날짜로 넘어감\n",
    "            state = 2\n",
    "          elif condition_date.date() == date:\n",
    "            if condition_hourgrp > hourgrp:       # 다음 그룹시간으로 넘어감\n",
    "              state = 2\n",
    "            elif condition_hourgrp == hourgrp:    # 맞는 데이터\n",
    "              state = 1\n",
    "\n",
    "        if state == 0:                  # 계속 탐색\n",
    "          condition_idx += 1\n",
    "        elif state == 1:                # 데이터 찾음\n",
    "          # 이전 Condition 정보와 나중 Condition 정보를 합친다\n",
    "          prev_conditions = np.array(prev_conditions) + np.array(condition_data)\n",
    "          data_list[idx, col_start_idx:col_end_idx] = prev_conditions\n",
    "          condition_idx += 1\n",
    "          break\n",
    "        elif state == 2:                # 맞는 데이터가 없음\n",
    "          break\n",
    "\n",
    "      prev_person_id = person_id\n",
    "\n",
    "    feature_ary = data_list\n",
    "    feature_key_df = pd.DataFrame(key_list, columns=['PERSON_ID', 'DATE', 'HOURGRP'])\n",
    "    print(\"data_loader make_person_sequence time:\", time.time() - start_time)\n",
    "    return feature_ary, feature_key_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ary, feature_key_df = make_person_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_data():\n",
    "    start_time = time.time()\n",
    "    # 빠른 서치를 위하여 데이터 정렬\n",
    "    # 가장 마지막 시점이 먼저 오도록 반대로 정렬\n",
    "    global cohort_df, feature_key_df, feature_ary\n",
    "    cohort_df = cohort_df.sort_values(['SUBJECT_ID', 'COHORT_END_DATE'], ascending=[True, False])\n",
    "    feature_key_df = feature_key_df.sort_values(['PERSON_ID', 'DATE', 'HOURGRP'], ascending=[True, False, False])\n",
    "    feature_ary = feature_ary[feature_key_df.index]\n",
    "    feature_key_ary = feature_key_df.values\n",
    "    \n",
    "    cols = ['SUBJECT_ID', 'COHORT_END_DATE']\n",
    "    if is_train:\n",
    "      cols.append('LABEL')\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    key_list = []\n",
    "    feature_idx = 0\n",
    "\n",
    "    for row in cohort_df[cols].values:\n",
    "      subject_id = row[0]\n",
    "      cohort_end_date = row[1]\n",
    "\n",
    "      # key에 맞는 data feature 찾는다\n",
    "      while True:\n",
    "        person_id, feature_date, feature_hourgrp = feature_key_ary[feature_idx]\n",
    "        feature_row = feature_ary[feature_idx]\n",
    "\n",
    "        feature_datetime = datetime(feature_date.year,\n",
    "                                    feature_date.month,\n",
    "                                    feature_date.day,\n",
    "                                    feature_hourgrp * group_hour)\n",
    "        if person_id == subject_id and feature_datetime < cohort_end_date:\n",
    "          # 같은 환자이고 cohort_end_date보다 먼저 발생한 데이터이면\n",
    "          # 맞는 데이터\n",
    "          each_x_list = []\n",
    "          for timestep in range(timestep_per_data):\n",
    "            if feature_idx + timestep >= len(feature_ary):\n",
    "              break\n",
    "            timestep_person_id = feature_key_ary[feature_idx + timestep][0]\n",
    "            timestep_row = feature_ary[feature_idx + timestep]\n",
    "            if timestep_person_id == subject_id:\n",
    "              timestep_data = timestep_row\n",
    "              each_x_list.append(timestep_data)\n",
    "            else:\n",
    "              break\n",
    "          # 가장 나중 데이터부터 each_x_list에 넣었으니 데이터에 넣을땐 반대로\n",
    "          x_list.append(np.array(each_x_list)[::-1])\n",
    "          break\n",
    "        elif person_id > subject_id:\n",
    "          # 데이터를 못찾음. 다음 환자로 넘어가버렸다\n",
    "          print(\"Person's data not found\", subject_id)\n",
    "          feature_data = feature_row\n",
    "          x_list.append(np.array([[0] * len(feature_data)]))\n",
    "          break\n",
    "        else:\n",
    "          # 탐색이 더 필요함\n",
    "          feature_idx += 1\n",
    "\n",
    "      # y 추가\n",
    "      if is_train:\n",
    "        label = row[2]\n",
    "        y_list.append(label)\n",
    "\n",
    "      key_list.append((row[0], row[1]))\n",
    "\n",
    "    x = np.array(x_list)\n",
    "    y = np.array(y_list) if is_train else None\n",
    "    key = pd.DataFrame(key_list, columns=['SUBJECT_ID', 'COHORT_END_DATE'])\n",
    "    print(\"X\", x.shape)\n",
    "    if y is not None:\n",
    "      print(\"Y\", y.shape)\n",
    "    print(\"Key\", key.shape)\n",
    "    print(\"data_loader make_data time:\", time.time() - start_time)\n",
    "    return x,y,key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,key = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _stratified_shuffle():\n",
    "    whole_patient = set(key.SUBJECT_ID.unique())\n",
    "    true_patient = set(key.loc[np.where(y == 1)[0], ].SUBJECT_ID.unique())\n",
    "    false_patient = whole_patient - true_patient\n",
    "\n",
    "    true_train_patient, true_valid_patient = train_test_split(list(true_patient),\n",
    "                                                              train_size=(1 - valid_size),\n",
    "                                                              test_size=valid_size,\n",
    "                                                              random_state=data_split_random_seed)\n",
    "\n",
    "    false_train_patient, false_valid_patient = train_test_split(list(false_patient),\n",
    "                                                                train_size=(1 - valid_size),\n",
    "                                                                test_size=valid_size,\n",
    "                                                                random_state=data_split_random_seed)\n",
    "\n",
    "    true_train_x = x[key.SUBJECT_ID.isin(true_train_patient)]\n",
    "    true_valid_x = x[key.SUBJECT_ID.isin(true_valid_patient)]\n",
    "    true_train_y = y[key.SUBJECT_ID.isin(true_train_patient)]\n",
    "    true_valid_y = y[key.SUBJECT_ID.isin(true_valid_patient)]\n",
    "    \n",
    "    true_x = true_train_x[np.where(true_train_y==1)]\n",
    "    \n",
    "    true_train_x = x[key.SUBJECT_ID.isin(true_train_patient)]\n",
    "    true_train_x = x[key.SUBJECT_ID.isin(true_train_patient)]\n",
    "    true_train_x = x[key.SUBJECT_ID.isin(true_train_patient)]\n",
    "    \n",
    "    train_patient = np.concatenate([true_train_patient, false_train_patient])\n",
    "    valid_patient = np.concatenate([true_valid_patient, false_valid_patient])\n",
    "\n",
    "    train_x = x[key.SUBJECT_ID.isin(train_patient)]\n",
    "    train_y = y[key.SUBJECT_ID.isin(train_patient)]\n",
    "\n",
    "    valid_x = x[key.SUBJECT_ID.isin(valid_patient)]\n",
    "    valid_y = y[key.SUBJECT_ID.isin(valid_patient)]\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-de325fe0f9e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.where(y==1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input((None, x3.shape[2]))\n",
    "x = model_input\n",
    "\n",
    "x = Masking(mask_value=0.0)(x)\n",
    "\n",
    "x = Dense(16, activation = 'tanh')(x)\n",
    "rnn_layers = [32]\n",
    "for idx, node in enumerate(rnn_layers):\n",
    "  return_sequences = False if idx == len(rnn_layers) - 1 else True\n",
    "\n",
    "  x = GRU(node, activation = 'relu',\n",
    "          return_sequences=return_sequences)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(1, kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "\n",
    "model_output = Activation('sigmoid')(x)\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3  =pad_sequences(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(592,)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GRU, Input, Masking, Dropout, Dense, Activation, BatchNormalization\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_time = []\n",
    "Xt_static = []\n",
    "yt_time = []\n",
    "\n",
    "## positive data gathering\n",
    "for i in range(len(list_time_X)):\n",
    "    Xt_time_i = list_time_X[i]\n",
    "    yt_time_i = list_time_y[i]\n",
    "    Xt, yt = time_series_generator(Xt_time_i,yt_time_i)\n",
    "    time_static_data = list_time_static[i]\n",
    "\n",
    "    for n in range(len(Xt)):\n",
    "        Xt_time.append(Xt[n])\n",
    "        Xt_static.append(time_static_data)\n",
    "        yt_time.append(yt[n])\n",
    "\n",
    "positive_index = [i for i,result in enumerate(yt_time) if result==1]\n",
    "negative_index = [i for i,result in enumerate(yt_time) if result==0]\n",
    "\n",
    "positive_x = [Xt_time[i] for i in positive_index]\n",
    "positive_x_static = [Xt_static[i] for i in positive_index]\n",
    "positive_y = [yt_time[i] for i in positive_index]\n",
    "\n",
    "negative_x = [Xt_time[i] for i in negative_index]\n",
    "negative_x_static = [Xt_static[i] for i in negative_index]\n",
    "negative_y = [yt_time[i] for i in negative_index]\n",
    "\n",
    "true_X_time = np.array(positive_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "true_X_static = np.array(positive_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "true_y_total = np.array(positive_y, dtype=\"float32\").reshape(-1,)\n",
    "false_X_time = np.array(negative_x, dtype=\"float32\").reshape(-1,window,feature)\n",
    "false_X_static = np.array(negative_x_static, dtype=\"float32\").reshape(-1,38)\n",
    "false_y_total = np.array(negative_y, dtype=\"float32\").reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 592 samples, validate on 592 samples\n",
      "Epoch 1/4\n",
      "592/592 [==============================] - 2s 3ms/sample - loss: 0.5255 - acc: 0.9139 - val_loss: 0.5338 - val_acc: 0.9375\n",
      "Epoch 2/4\n",
      "592/592 [==============================] - 2s 3ms/sample - loss: 0.5190 - acc: 0.9139 - val_loss: 0.5214 - val_acc: 0.9375\n",
      "Epoch 3/4\n",
      "592/592 [==============================] - 2s 3ms/sample - loss: 0.5015 - acc: 0.9409 - val_loss: 0.5076 - val_acc: 0.9375\n",
      "Epoch 4/4\n",
      "592/592 [==============================] - 2s 3ms/sample - loss: 0.4874 - acc: 0.9426 - val_loss: 0.4929 - val_acc: 0.9375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19c87f4da20>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x3, y,\n",
    "                   epochs=4,\n",
    "                   verbose=1,\n",
    "                   batch_size=100,\n",
    "                   validation_data=[x3,y],\n",
    "                   callbacks=[]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, None, 122)]       0         \n",
      "_________________________________________________________________\n",
      "masking_3 (Masking)          (None, None, 122)         0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, None, 16)          1968      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 32)                4704      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 6,833\n",
      "Trainable params: 6,769\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=1, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-232-5b5f4eec0e48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_stratified_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-231-0b07382f09b4>\u001b[0m in \u001b[0;36m_stratified_shuffle\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                             \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalid_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                                                             \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m                                                             random_state=data_split_random_seed)\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m   false_train_patient, false_valid_patient = train_test_split(list(false_patient),\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorgpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2098\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2099\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2100\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tensorgpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1780\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1781\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1782\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1783\u001b[0m         )\n\u001b[0;32m   1784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=1, test_size=0.2 and train_size=0.8, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = _stratified_shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_sampling_size = 600\n",
    "fraction_per_case = 0.2\n",
    "true_false_ratio = 1\n",
    "\n",
    "class file_generator():\n",
    "    \"Generating model\"\n",
    "    def __init__(self, window, time_it, feature,\n",
    "                 true_X_time, true_X_static,true_y_total,\n",
    "                 false_X_time, false_X_static, false_y_total,\n",
    "                 list_time_Xn, list_time_yn, list_time_nstatic, \n",
    "                 random_sampling_size, fraction_per_case, true_false_ratio):\n",
    "        self.window = window\n",
    "        self.time_it = time_it\n",
    "        self.feature = feature\n",
    "        self.true_X_time = true_X_time\n",
    "        self.true_X_static = true_X_static\n",
    "        self.true_y_total = true_y_total\n",
    "        self.false_X_time = false_X_time\n",
    "        self.false_X_static = false_X_static\n",
    "        self.false_y_total = false_y_total\n",
    "        self.list_time_Xn = list_time_Xn\n",
    "        self.list_time_yn = list_time_yn\n",
    "        self.list_time_nstatic = list_time_nstatic\n",
    "        self.random_sampling_size = random_sampling_size\n",
    "        self.fraction_per_case = fraction_per_case\n",
    "        self.true_false_ratio = true_false_ratio\n",
    "\n",
    "\n",
    "    def get_data(self, index):\n",
    "        Xn_time = []\n",
    "        Xn_static = []\n",
    "        yn_time = []\n",
    "\n",
    "        random_sampling = np.random.choice(len(self.list_time_Xn), size= self.random_sampling_size)\n",
    "        ## negative data gathering\n",
    "        for i in random_sampling:\n",
    "            X_time_n = self.list_time_Xn[i]\n",
    "            y_time_n = self.list_time_yn[i]\n",
    "            Xn, yn = self.time_series_generator(X_time_n,y_time_n)\n",
    "            time_static_data = self.list_time_nstatic[i]\n",
    "\n",
    "            for n in range(len(Xn)):\n",
    "                Xn_time.append(Xn[n])\n",
    "                Xn_static.append(time_static_data)\n",
    "                yn_time.append(yn[n])\n",
    "\n",
    "        np_Xn_time = np.array(Xn_time, dtype=\"float32\").reshape(-1,self.window,self.feature)\n",
    "        np_Xn_static = np.array(Xn_static, dtype=\"float32\").reshape(-1,38)\n",
    "        np_yn_time = np.array(yn_time, dtype=\"float32\").reshape(-1,)\n",
    "\n",
    "        batch_false_list = np.random.choice(len(self.false_X_time), size=int(np.floor(len(self.true_X_time)*self.true_false_ratio)))\n",
    "\n",
    "        batch_false_X = self.false_X_time[batch_false_list]\n",
    "        batch_false_Xs = self.false_X_static[batch_false_list]\n",
    "        batch_false_y = self.false_y_total[batch_false_list]\n",
    "\n",
    "        batch_X=np.concatenate([np_Xn_time,batch_false_X, self.true_X_time],axis=0)\n",
    "        batch_static_X=np.concatenate([np_Xn_static,batch_false_Xs, self.true_X_static], axis=0)\n",
    "        batch_y=np.concatenate([np_yn_time,batch_false_y, self.true_y_total],axis=0)\n",
    "\n",
    "        batch_X, batch_static_X, batch_y = self.shuffling(batch_X, batch_static_X, batch_y)\n",
    "\n",
    "        return batch_X, batch_static_X, batch_y\n",
    "\n",
    "\n",
    "    def time_series_generator(self, x,y):\n",
    "        Xn=[]\n",
    "        yn=[]\n",
    "        size_x = len(x)-self.time_it\n",
    "        random_selection = np.random.choice(size_x, size = int(np.floor(size_x*self.fraction_per_case)))\n",
    "        for n in random_selection:\n",
    "            if n+1>self.window:\n",
    "                X_train=x[n+1-self.window:n+1]\n",
    "            else:\n",
    "                X_train=x[0:n+1]\n",
    "                X_train=np.pad(X_train, mode='constant', pad_width=((0,self.window-X_train.shape[0]),(0,0)),\\\n",
    "                               constant_values=-5)\n",
    "\n",
    "            Xn.append(X_train)\n",
    "            y_train=y[n+self.time_it]\n",
    "            yn.append(y_train)\n",
    "\n",
    "        return Xn, yn\n",
    "\n",
    "    def shuffling(self, a, b, c):\n",
    "        assert len(a) == len(b)\n",
    "        p = np.random.permutation(len(a))\n",
    "        return a[p], b[p], c[p]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
