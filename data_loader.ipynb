{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from docker.src.measurement_stat import MEASUREMENT_SOURCE_VALUE_STATS\n",
    "from datetime import datetime, timedelta, time as datetime_time, timezone\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "VALUE_MAP = ['HR','RR','SpO2','Pulse','Temp','ABPm','ABPd','ABPs','NBPm','NBPs','NBPd','SPO2-%','SPO2-R',\n",
    "'Resp','PVC','ST-II','etCO2','SpO2 r','imCO2','ST-V1','ST-I','ST-III','ST-aVF','ST-aVL','ST-aVR',\n",
    "'awRR','CVPm','AoM','ST-V2','ST-V3','ST-V4','ST-V5','ST-V6','SpO2T','T1','TV','Cdyn','PEEP','RRaw',\n",
    "'TVin','inO2','AoD','AoS','InsTi','MINVOL','MnAwP','PIP','MVin','PB','Poccl','Pplat',\n",
    "'MV','Patm','Ppeak','Rinsp','ST-V','sInsTi','sPEEP','sTV','sTrig','sPSV','Rexp','highP',\n",
    "'sAPkFl','sAWRR','sFIO2','sPIF','sMV','sO2','sRisTi','ARTd','ARTm','ARTs','PAPm','sSIMV']\n",
    "\n",
    "MEASUREMENT_NORMALIZATION = ['mean', 'predefined']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='./data/train'\n",
    "common_path='./data/volume'\n",
    "\n",
    "task_path='./data/volume/local_test'\n",
    "# os.mkdir(task_path)\n",
    "is_train=True\n",
    "\n",
    "group_hour=1\n",
    "timestep_per_data=128\n",
    "\n",
    "measurement_normalize='mean'\n",
    "\n",
    "condition_min_limit=0\n",
    "condition_group=False\n",
    "\n",
    "valid_size=0.2\n",
    "data_split_random_seed=1235\n",
    "pytest=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.011013984680175781\n",
      "data_loader extract_person time: 0.0070133209228515625\n",
      "data_loader extract_condition time: 0.006035566329956055\n",
      "data_loader extract_measurement time: 2.281010866165161\n"
     ]
    }
   ],
   "source": [
    "  def extract_outcome_cohort():\n",
    "    start_time = time.time()\n",
    "    cohort_df = pd.read_csv(os.path.join(data_path, 'OUTCOME_COHORT.csv'), encoding='windows-1252')\n",
    "\n",
    "    cohort_df.COHORT_START_DATE = pd.to_datetime(cohort_df.COHORT_START_DATE)\n",
    "    cohort_df.COHORT_END_DATE = pd.to_datetime(cohort_df.COHORT_END_DATE)\n",
    "    print(\"data_loader extract_outcome_cohort time:\", time.time() - start_time)\n",
    "    return cohort_df\n",
    "\n",
    "  def extract_person():\n",
    "    start_time = time.time()\n",
    "    person_df = pd.read_csv(os.path.join(data_path, 'PERSON_NICU.csv'), encoding='windows-1252')\n",
    "    person_df = pd.concat([\n",
    "        person_df[['PERSON_ID', 'BIRTH_DATETIME']],\n",
    "        pd.get_dummies(person_df.GENDER_SOURCE_VALUE, prefix='gender')\n",
    "    ], axis=1)\n",
    "\n",
    "    # 생일 컬럼 타입 설정\n",
    "    person_df.BIRTH_DATETIME = pd.to_datetime(person_df.BIRTH_DATETIME, utc=True)\n",
    "    # 여성/남성 컬럼 1개로 처리\n",
    "    person_df.rename(columns={'gender_M': 'GENDER'}, inplace=True)\n",
    "    if 'gender_F' in person_df.columns:\n",
    "      del person_df['gender_F']\n",
    "\n",
    "    print(\"data_loader extract_person time:\", time.time() - start_time)\n",
    "    return person_df\n",
    "\n",
    "  def extract_condition():\n",
    "    start_time = time.time()\n",
    "    condition_df = pd.read_csv(os.path.join(data_path, 'CONDITION_OCCURRENCE_NICU.csv'), encoding='windows-1252',\n",
    "                               usecols=['PERSON_ID', 'CONDITION_SOURCE_VALUE', 'CONDITION_START_DATETIME'])\n",
    "    # Null 이거나 값이 빈 것을 날림\n",
    "    condition_df = condition_df[pd.notnull(condition_df.CONDITION_SOURCE_VALUE)]\n",
    "    condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.str.len() > 0]\n",
    "\n",
    "    if condition_group:\n",
    "      condition_df.CONDITION_SOURCE_VALUE = condition_df.CONDITION_SOURCE_VALUE.str.slice(stop=3)\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    condition_df.CONDITION_START_DATETIME = pd.to_datetime(condition_df.CONDITION_START_DATETIME, utc=True)\n",
    "\n",
    "    print(\"data_loader extract_condition time:\", time.time() - start_time)\n",
    "    return condition_df\n",
    "\n",
    "  def extract_measurement():\n",
    "    start_time = time.time()\n",
    "    measurement_df = pd.read_csv(os.path.join(data_path, 'MEASUREMENT_NICU.csv'), \n",
    "                                 encoding='windows-1252',\n",
    "                                 usecols=['PERSON_ID', 'MEASUREMENT_DATETIME',\n",
    "                                          'MEASUREMENT_SOURCE_VALUE', 'VALUE_AS_NUMBER']\n",
    "                                 )\n",
    "#     if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "#       # source_value 맵핑\n",
    "#       source_value_invert_map = {}\n",
    "#       for new_value in MEASUREMENT_SOURCE_VALUE_MAP:\n",
    "#         for table_value in MEASUREMENT_SOURCE_VALUE_MAP[new_value]:\n",
    "#           source_value_invert_map[table_value] = new_value\n",
    "#       measurement_df.MEASUREMENT_SOURCE_VALUE = measurement_df.MEASUREMENT_SOURCE_VALUE.replace(source_value_invert_map)\n",
    "\n",
    "      # 맵핑이된 정보만 남긴다\n",
    "    measurement_df = measurement_df[measurement_df.MEASUREMENT_SOURCE_VALUE.isin(VALUE_MAP)]\n",
    "\n",
    "    # 컬럼 타입 설정\n",
    "    measurement_df.MEASUREMENT_DATETIME = pd.to_datetime(measurement_df.MEASUREMENT_DATETIME, utc=True)\n",
    "\n",
    "    # source_value별 평균값 추출\n",
    "    if is_train:\n",
    "      measurement_mean_df = measurement_df.groupby('MEASUREMENT_SOURCE_VALUE').VALUE_AS_NUMBER.mean()\n",
    "      measurement_mean_df.to_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "    else:\n",
    "      # inference일 경우 저장된 걸 불러온다\n",
    "      measurement_mean_df = pd.read_pickle(os.path.join(common_path, 'measurement_mean.pkl'))\n",
    "\n",
    "    print(\"data_loader extract_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df, measurement_mean_df\n",
    "cohort_df = extract_outcome_cohort()\n",
    "person_df = extract_person()\n",
    "condition_df = extract_condition()\n",
    "measurement_df,measurement_mean_df = extract_measurement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def groupby_hour_condition( condition_df):\n",
    "    start_time = time.time()\n",
    "\n",
    "    condition_df['CONDITION_DATE'] = condition_df.CONDITION_START_DATETIME.dt.date\n",
    "    condition_df['CONDITION_DATE'] = pd.to_datetime(condition_df.CONDITION_DATE, utc=True)\n",
    "\n",
    "    if is_train and condition_min_limit > 0:\n",
    "      condition_group = condition_df.groupby('CONDITION_SOURCE_VALUE').PERSON_ID.count()\n",
    "      condition_group = condition_group[condition_group > condition_min_limit].index\n",
    "\n",
    "      condition_df = condition_df[condition_df.CONDITION_SOURCE_VALUE.isin(condition_group)]\n",
    "\n",
    "    # 진단은 시간이 없다. 당일의 마지막에 진단 받은걸로 가정한다\n",
    "    condition_df['HOURGRP'] = 23 // group_hour\n",
    "\n",
    "    group_cols = ['PERSON_ID', 'CONDITION_DATE', 'HOURGRP', 'CONDITION_SOURCE_VALUE']\n",
    "\n",
    "    condition_df['DUMMY'] = condition_df['CONDITION_SOURCE_VALUE']\n",
    "    condition_df = condition_df.groupby(group_cols) \\\n",
    "        .DUMMY.count().unstack().reset_index().fillna(0)\n",
    "\n",
    "    condition_df = condition_df.rename(columns={'CONDITION_DATE': 'DATE'})\n",
    "\n",
    "    condition_col_filename = os.path.join(task_path, 'condition_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(condition_col_filename, np.array(condition_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      condition_cols = np.load(condition_col_filename, allow_pickle=True)\n",
    "      new_condition_list = []\n",
    "      for col in condition_cols:\n",
    "        if col in condition_df.columns:\n",
    "          new_condition_list.append(condition_df[col])\n",
    "        else:\n",
    "          new_condition_list.append(pd.Series([0] * condition_df.shape[0]))\n",
    "\n",
    "      condition_df = pd.concat(new_condition_list, axis=1)\n",
    "      condition_df.columns = condition_cols\n",
    "    print(\"data_loader groupby_hour_condition time:\", time.time() - start_time)\n",
    "    return condition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader groupby_hour_condition time: 0.01300191879272461\n"
     ]
    }
   ],
   "source": [
    "condition_df = groupby_hour_condition(condition_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _clip_measurement(measurement_source_value, value_as_number):\n",
    "    if value_as_number > MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['95%']\n",
    "    elif value_as_number < MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']:\n",
    "      value_as_number = MEASUREMENT_SOURCE_VALUE_STATS[measurement_source_value]['5%']\n",
    "    return value_as_number\n",
    "\n",
    "  def groupby_hour_measurement(measurement_df):\n",
    "    start_time = time.time()\n",
    "    # timestamp로 join 하기 위하여 시간 포맷을 utc로 통일\n",
    "    measurement_df['MEASUREMENT_DATE'] = measurement_df.MEASUREMENT_DATETIME.dt.date\n",
    "    measurement_df['MEASUREMENT_DATE'] = pd.to_datetime(measurement_df.MEASUREMENT_DATE, utc=True)\n",
    "\n",
    "    measurement_df['MEASUREMENT_HOUR'] = measurement_df.MEASUREMENT_DATETIME.dt.hour\n",
    "    measurement_df['MEASUREMENT_HOURGRP'] = measurement_df.MEASUREMENT_HOUR // group_hour\n",
    "\n",
    "    # 평균값 이용하여 Normalize\n",
    "    if measurement_normalize == MEASUREMENT_NORMALIZATION[0]:\n",
    "      measurement_df = pd.merge(measurement_df,\n",
    "                                measurement_mean_df.reset_index().rename(\n",
    "                                    columns={'VALUE_AS_NUMBER': 'MEAN_VALUE'}),\n",
    "                                on='MEASUREMENT_SOURCE_VALUE', how='left')\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.VALUE_AS_NUMBER / measurement_df.MEAN_VALUE\n",
    "    # 생체신호 범위를 이용하여 Normalize\n",
    "    elif measurement_normalize == MEASUREMENT_NORMALIZATION[1]:\n",
    "      measurement_df.VALUE_AS_NUMBER = measurement_df.apply(lambda row:\n",
    "                                                            _clip_measurement(\n",
    "                                                                row['MEASUREMENT_SOURCE_VALUE'],\n",
    "                                                                row['VALUE_AS_NUMBER']),\n",
    "                                                            axis=1)\n",
    "\n",
    "      # TODO\n",
    "    group_cols = ['PERSON_ID', 'MEASUREMENT_DATE', 'MEASUREMENT_HOURGRP', 'MEASUREMENT_SOURCE_VALUE']\n",
    "    agg_list = ['count', 'min', 'max', 'mean', 'std', 'var']\n",
    "    measurement_df['VALUE_DIFF'] = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.diff()\n",
    "\n",
    "    measurement_diff_df = pd.pivot_table(measurement_df, \n",
    "                                         values='VALUE_DIFF', index=group_cols[:-1],\n",
    "                                         columns='MEASUREMENT_SOURCE_VALUE', aggfunc=['mean','max','min'])\n",
    "\n",
    "    measurement_diff_df.columns = [('diff', '{}_{}'.format(v[0],v[1])) for v in measurement_diff_df.columns]\n",
    "\n",
    "    measurement_df = measurement_df.groupby(group_cols).VALUE_AS_NUMBER.agg(agg_list).fillna(0).unstack().fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    measurement_df = pd.concat([measurement_df, measurement_diff_df], axis=1).reset_index()\n",
    "\n",
    "    if measurement_df.isnull().sum().sum() >0:\n",
    "        print(\"there is Na after interpolation\")\n",
    "        measurement_df = measurement_df.fillna(0)\n",
    "        \n",
    "    # 사용한 후 삭제\n",
    "    del measurement_diff_df\n",
    "    # 컬럼 이름 정제 (그룹화 하기 쉽게)\n",
    "    new_cols = []\n",
    "    for col in measurement_df.columns:\n",
    "      \n",
    "      if col[1] == '':\n",
    "        new_cols.append(col[0])\n",
    "      elif col[0] in agg_list + ['diff']:\n",
    "        new_cols.append((col[1], col[0]))\n",
    "    measurement_df.columns = new_cols\n",
    "\n",
    "#     #minmax scale\n",
    "#     scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "#     scaler = scaler.fit(measurement_df.iloc[:,3:])\n",
    "#     measurement_df.iloc[:,3:] = scaler.transform(measurement_df.iloc[:,3:])\n",
    "    \n",
    "    measurement_df = measurement_df.rename(columns={'MEASUREMENT_DATE': 'DATE',\n",
    "                                                    'MEASUREMENT_HOURGRP': 'HOURGRP'})\n",
    "\n",
    "    measurement_col_filename = os.path.join(task_path, 'measurement_cols.npy')\n",
    "    if is_train:\n",
    "      # 컬럼 이름 저장\n",
    "      np.save(measurement_col_filename, np.array(measurement_df.columns))\n",
    "    else:\n",
    "      # 컬럼 로드\n",
    "      measurement_cols = np.load(measurement_col_filename, allow_pickle=True)\n",
    "      new_measurement_list = []\n",
    "      for col in measurement_cols:\n",
    "        if col in measurement_df.columns:\n",
    "          new_measurement_list.append(measurement_df[col])\n",
    "        else:\n",
    "          new_measurement_list.append(pd.Series([0] * measurement_df.shape[0]))\n",
    "\n",
    "      measurement_df = pd.concat(new_measurement_list, axis=1)\n",
    "      measurement_df.columns = measurement_cols\n",
    "    print(\"data_loader groupby_hour_measurement time:\", time.time() - start_time)\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 7.197989463806152\n"
     ]
    }
   ],
   "source": [
    "measurement_df = groupby_hour_measurement(measurement_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder 작업중 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.009999513626098633\n",
      "data_loader extract_person time: 0.006041765213012695\n",
      "data_loader extract_condition time: 0.00601649284362793\n",
      "data_loader extract_measurement time: 2.2839856147766113\n",
      "data_loader groupby_hour_condition time: 0.0169980525970459\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 6.957036018371582\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               7020      \n",
      "=================================================================\n",
      "Total params: 29,228\n",
      "Trainable params: 29,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "data_loader autoencoder.predict time: 2.035015106201172\n",
      "data_loader make_person_sequence time: 0.07996082305908203\n",
      "X (1592,)\n",
      "Y (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.19301915168762207\n",
      "on_split\n",
      "data_loader split_data time: 0.03295612335205078\n"
     ]
    }
   ],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "import os\n",
    "task_path='./data/volume/local_test'\n",
    "data_loader = DataLoader(data_path=os.path.join('./data', 'train'),\n",
    "                         common_path=os.path.join(\"./data\", 'volume'),\n",
    "                         measurement_normalize='mean',\n",
    "                         is_train = True,\n",
    "                         task_path=task_path,\n",
    "                        switch = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               7020      \n",
      "=================================================================\n",
      "Total params: 29,228\n",
      "Trainable params: 29,228\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      " - 0s - loss: 726.2963 - val_loss: 717.0902\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 717.09021, saving model to ./data/volume/local_test\\encoder-01-717.090210.hdf5\n",
      "Epoch 2/10\n",
      " - 0s - loss: 721.8182 - val_loss: 714.8348\n",
      "\n",
      "Epoch 00002: val_loss improved from 717.09021 to 714.83484, saving model to ./data/volume/local_test\\encoder-02-714.834839.hdf5\n",
      "Epoch 3/10\n",
      " - 0s - loss: 719.5919 - val_loss: 712.9244\n",
      "\n",
      "Epoch 00003: val_loss improved from 714.83484 to 712.92444, saving model to ./data/volume/local_test\\encoder-03-712.924438.hdf5\n",
      "Epoch 4/10\n",
      " - 0s - loss: 717.7097 - val_loss: 711.1398\n",
      "\n",
      "Epoch 00004: val_loss improved from 712.92444 to 711.13983, saving model to ./data/volume/local_test\\encoder-04-711.139832.hdf5\n",
      "Epoch 5/10\n",
      " - 0s - loss: 715.9441 - val_loss: 710.1699\n",
      "\n",
      "Epoch 00005: val_loss improved from 711.13983 to 710.16992, saving model to ./data/volume/local_test\\encoder-05-710.169922.hdf5\n",
      "Epoch 6/10\n",
      " - 0s - loss: 714.9875 - val_loss: 709.7444\n",
      "\n",
      "Epoch 00006: val_loss improved from 710.16992 to 709.74445, saving model to ./data/volume/local_test\\encoder-06-709.744446.hdf5\n",
      "Epoch 7/10\n",
      " - 0s - loss: 714.5642 - val_loss: 709.5330\n",
      "\n",
      "Epoch 00007: val_loss improved from 709.74445 to 709.53302, saving model to ./data/volume/local_test\\encoder-07-709.533020.hdf5\n",
      "Epoch 8/10\n",
      " - 0s - loss: 714.3526 - val_loss: 709.4155\n",
      "\n",
      "Epoch 00008: val_loss improved from 709.53302 to 709.41553, saving model to ./data/volume/local_test\\encoder-08-709.415527.hdf5\n",
      "Epoch 9/10\n",
      " - 0s - loss: 714.2343 - val_loss: 709.3340\n",
      "\n",
      "Epoch 00009: val_loss improved from 709.41553 to 709.33405, saving model to ./data/volume/local_test\\encoder-09-709.334045.hdf5\n",
      "Epoch 10/10\n",
      " - 0s - loss: 714.1519 - val_loss: 709.2733\n",
      "\n",
      "Epoch 00010: val_loss improved from 709.33405 to 709.27325, saving model to ./data/volume/local_test\\encoder-10-709.273254.hdf5\n"
     ]
    }
   ],
   "source": [
    "from docker.src.model import Autoencoder\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "\n",
    "callbacks = [\n",
    "ModelCheckpoint(filepath=os.path.join(task_path, 'encoder-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            verbose=True\n",
    "),\n",
    "EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=2, mode='auto')\n",
    "]\n",
    "\n",
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-778e9d08c7a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'load_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import model_from_json \n",
    "\n",
    "\n",
    "model_path = tf.train.latest_checkpoint(task_path)\n",
    "if model_path is None:\n",
    "  file_name = sorted([file_name for file_name in os.listdir(task_path) if file_name.endswith('.hdf5') and file_name.startswith('encoder')])[-1]\n",
    "  model_path = os.path.join(task_path, file_name)\n",
    "\n",
    "model = model.load_weights(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'encoder-10-725.665955.hdf5'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                      train_size=(1 - valid_size),\n",
    "                                                      test_size=valid_size,\n",
    "                                                      random_state=data_split_random_seed)\n",
    "\n",
    "input_img = Input(shape=(train_measure.iloc[:,3:].shape[1],))\n",
    "layer1=autoen.model.layers[1]\n",
    "layer2=autoen.model.layers[2]\n",
    "\n",
    "encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "output=encoder.predict(train_measure.iloc[:,3:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 1.9231129 , ..., 2.1713006 , 0.4146613 ,\n",
       "        2.4669633 ],\n",
       "       [0.        , 0.        , 1.7895968 , ..., 2.17478   , 0.41060454,\n",
       "        2.4875143 ],\n",
       "       [0.        , 0.        , 1.824985  , ..., 2.1258125 , 0.11797369,\n",
       "        2.0025125 ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 1.8242193 , ..., 2.1266065 , 0.11741423,\n",
       "        2.0028756 ],\n",
       "       [0.        , 0.        , 1.8244132 , ..., 2.126528  , 0.11736455,\n",
       "        2.0026877 ],\n",
       "       [0.        , 0.        , 1.8885305 , ..., 2.1614344 , 0.43662977,\n",
       "        2.492919  ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_img = Input(shape=(self.train_measure.shape[1],))\n",
    "    layer1=self.model.layers[1]\n",
    "    layer2=self.model.layers[2]\n",
    "\n",
    "    encoder= Model(input_img, layer2(layer1(input_img)))\n",
    "    output=encoder.predict(total_measure)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder():\n",
    "    train_measure, valid_measure = train_test_split(measurement_df,\n",
    "                                                          train_size=(1 - valid_size),\n",
    "                                                          test_size=valid_size,\n",
    "                                                          random_state=data_split_random_seed)\n",
    "    autoen = Autoencoder(train_measure.iloc[:,3:])\n",
    "    autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "    self.model = autoen\n",
    "    output = autoen.predict(measurement_df.iloc[:,3:])\n",
    "    mesurement_df[:,3:] = output\n",
    "    return measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoen.train(train_measure.iloc[:,3:],\n",
    "             valid_measure.iloc[:,3:], \n",
    "             epochs = 10, \n",
    "             batch_size = int(np.floor(len(train_measure.iloc[:,3:]))),\n",
    "             verbose = 2,\n",
    "            callbacks = [])\n",
    "\n",
    "output = autoen.predict(measurement_df.iloc[:,3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.3929615020751953\n",
      "data_loader extract_person time: 0.025004863739013672\n",
      "data_loader extract_condition time: 0.031996965408325195\n",
      "data_loader extract_measurement time: 2.231036424636841\n",
      "data_loader groupby_hour_condition time: 0.010988235473632812\n",
      "condition_shape :  (55, 15)\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 6.492940425872803\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "encoder1 (Dense)             (None, 128)               13952     \n",
      "_________________________________________________________________\n",
      "encoder2 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "decoder1 (Dense)             (None, 108)               13932     \n",
      "=================================================================\n",
      "Total params: 44,396\n",
      "Trainable params: 44,396\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 1824 samples, validate on 457 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.2831 - val_loss: 0.0826\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.08259, saving model to ./data/volume/local_test\\encoder-01-0.08.hdf5\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.0747 - val_loss: 0.0737\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.08259 to 0.07367, saving model to ./data/volume/local_test\\encoder-02-0.07.hdf5\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.0647 - val_loss: 0.0640\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.07367 to 0.06402, saving model to ./data/volume/local_test\\encoder-03-0.06.hdf5\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0557 - val_loss: 0.0484\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.06402 to 0.04836, saving model to ./data/volume/local_test\\encoder-04-0.05.hdf5\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0447 - val_loss: 0.0417\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.04836 to 0.04168, saving model to ./data/volume/local_test\\encoder-05-0.04.hdf5\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0386 - val_loss: 0.0356\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.04168 to 0.03562, saving model to ./data/volume/local_test\\encoder-06-0.04.hdf5\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0320 - val_loss: 0.0315\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.03562 to 0.03149, saving model to ./data/volume/local_test\\encoder-07-0.03.hdf5\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0288 - val_loss: 0.0281\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03149 to 0.02812, saving model to ./data/volume/local_test\\encoder-08-0.03.hdf5\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0261 - val_loss: 0.0255\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02812 to 0.02551, saving model to ./data/volume/local_test\\encoder-09-0.03.hdf5\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0232 - val_loss: 0.0233\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02551 to 0.02329, saving model to ./data/volume/local_test\\encoder-10-0.02.hdf5\n",
      "data_loader autoencoder time: 5.81405234336853\n",
      "data_loader make_person_sequence time: 0.07797789573669434\n",
      "X (1592,)\n",
      "Y (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.201002836227417\n",
      "on_split\n",
      "data_loader split_data time: 0.04504966735839844\n"
     ]
    }
   ],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "data_path = data_path='./data'\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         is_train = True,\n",
    "                         task_path=task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from docker.src.data_loader import DataLoader\n",
    "from docker.src.model import SimpleRNNModel\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import datetime\n",
    "\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "\n",
    "task_log_path = os.path.join(log_path, 'local_test')\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_loader, fraction, repeat):\n",
    "        'Initialization'\n",
    "        self.xt, self.yt,self.nx, self.ny = data_loader()\n",
    "        self.fraction = fraction\n",
    "        self.repeat = repeat\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.nx) / 5))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "            #positive_valid patient중 negative data undersampling    \n",
    "        rand_false2 = np.random.choice(self.nx.shape[0], size=int(np.floor(self.nx.shape[0]*self.fraction)))\n",
    "        random_nx = self.nx[rand_false2]\n",
    "        random_ny = self.ny[rand_false2]\n",
    "        \n",
    "        xt = np.repeat(self.xt, self.repeat, axis=0)\n",
    "        yt = np.repeat(self.yt, self.repeat, axis=0)\n",
    "        \n",
    "        train_x = np.concatenate([xt,random_nx], axis=0)\n",
    "        train_y = np.concatenate([yt,random_ny], axis=0)\n",
    "            \n",
    "        if len(train_x) == len(train_y):\n",
    "            p = np.random.permutation(len(train_x))\n",
    "            train_x = train_x[p]\n",
    "            self.train_y = train_y[p]  \n",
    "        else:\n",
    "            print(\"there is non match\")\n",
    "        self.train_x = pad_sequences(train_x, padding='post', value=-5)\n",
    "        return (self.train_x, self.train_y)   \n",
    "    \n",
    "    def shape(self):\n",
    "        return self.train_x.shape\n",
    "    \n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'model-{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    TensorBoard(log_dir=task_log_path,\n",
    "                write_graph=True\n",
    "    ),\n",
    "     EarlyStopping(monitor='val_loss', min_delta=0, patience=20, verbose=2, mode='auto')\n",
    "\n",
    "]\n",
    " # data generation \n",
    "traingen = DataGenerator(data_loader.get_train_data,fraction = 0.1, repeat = 5)\n",
    "valid_gen = DataGenerator(data_loader.get_valid_data,fraction = 0.1, repeat = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_x shape (320, 100, 142)\n",
      "sample_y positive percents 0.765625\n",
      "time before model train 2020-01-04 01:44:13.812893\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, None, 142)         0         \n",
      "_________________________________________________________________\n",
      "masking_2 (Masking)          (None, None, 142)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 32)                16800     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 16,961\n",
      "Trainable params: 16,897\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 5s - loss: 0.6871 - accuracy: 0.5841 - val_loss: 0.5512 - val_accuracy: 0.7542\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.55122, saving model to ./data/volume/local_test\\model-01-0.55.hdf5\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.6699 - accuracy: 0.6616 - val_loss: 0.5126 - val_accuracy: 0.7548\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.55122 to 0.51262, saving model to ./data/volume/local_test\\model-02-0.51.hdf5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-5ba2638595f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m model.train(traingen, valid_gen, epochs=10, valid_steps = 10, \n\u001b[1;32m---> 12\u001b[1;33m             step_epoch = 10, verbose=2, callbacks=callbacks, workers=-1)\n\u001b[0m",
      "\u001b[1;32mD:\\hack\\prism3\\prism\\docker\\src\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, traingen, valid_gen, epochs, valid_steps, step_epoch, verbose, callbacks, workers)\u001b[0m\n\u001b[0;32m     52\u001b[0m     self.model.fit_generator(generator= traingen, validation_data=valid_gen,\n\u001b[0;32m     53\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m                     verbose=verbose, callbacks=callbacks,workers=workers)\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             \u001b[0mgenerator_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__len__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36miter_sequence_infinite\u001b[1;34m(seq)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \"\"\"\n\u001b[0;32m    591\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\utils\\data_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;34m\"\"\"Create a generator that iterate over the Sequence.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-ba5595b457c5>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"there is non match\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     81\u001b[0m                          .format(dtype, type(value)))\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[0mmultiarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'unsafe'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mcopyto\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_x,sample_y = traingen.__getitem__(1)\n",
    "print(\"sample_x shape\", sample_x.shape)\n",
    "print(\"sample_y positive percents\", sample_y.sum()/len(sample_y))\n",
    "\n",
    "print(\"time before model train\",datetime.datetime.now())\n",
    "model = SimpleRNNModel(shape=sample_x.shape[2])\n",
    "del sample_x #memory save \n",
    "\n",
    "# model train \n",
    "\n",
    "model.train(traingen, valid_gen, epochs=10, valid_steps = 10, \n",
    "            step_epoch = 10, verbose=2, callbacks=callbacks, workers=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
