{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\podkd\\.conda\\envs\\park\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM,GRU\n",
    "from keras.layers import Lambda\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop,adam\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Input, Dense, Masking, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "\n",
    "# from docker.src.model import DataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.metrics import f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measur = pd.read_csv(\"./data/train/MEASUREMENT_NICU.csv\", encoding='windows-1252')\n",
    "# condi = pd.read_csv(\"./data/train/CONDITION_OCCURRENCE_NICU.csv\", encoding='windows-1252')\n",
    "# outcome = pd.read_csv(\"./data/train/OUTCOME_COHORT.csv\", encoding='windows-1252')\n",
    "# person = pd.read_csv(\"./data/train/PERSON_NICU.csv\", encoding='windows-1252')\n",
    "\n",
    "# list1 =np.zeros([10,2])\n",
    "\n",
    "# list1[:,0]=range(10,20)\n",
    "# list1[:,1]=100000\n",
    "\n",
    "# for i,n in list1:\n",
    "#     measure_copy= measur[:int(n)].copy()\n",
    "#     measure_copy.PERSON_ID =int(i)\n",
    "#     measur = pd.concat([measur,measure_copy],axis=0)\n",
    "\n",
    "# measur.to_csv(\"./MEASUREMENT_NICU.csv\", encoding='windows-1252')\n",
    "\n",
    "# list1 =np.zeros([10,2])\n",
    "\n",
    "# list1[:,0]=range(10,20)\n",
    "# list1[:,1]=100000\n",
    "\n",
    "# condi_copy= condi.copy()\n",
    "\n",
    "# for i in range(10,20):\n",
    "#     condi_copy2= condi.copy()\n",
    "#     condi_copy2.PERSON_ID =int(i)\n",
    "#     condi_copy = pd.concat([condi_copy,condi_copy2],axis=0)\n",
    "    \n",
    "\n",
    "# condi_copy.to_csv(\"./CONDITION_OCCURRENCE_NICU.csv\", encoding='windows-1252')\n",
    "\n",
    "# import random\n",
    "# np.random.choice(2,1,p=[0.8,0.2])[0]\n",
    "\n",
    "# for i in range(10,20):\n",
    "#     outcome_copy = outcome[:100].copy()\n",
    "#     outcome_copy.SUBJECT_ID =int(i)\n",
    "#     if np.random.choice(2,1,p=[0.6,0.4])[0] == 1:\n",
    "#         print(\"1\")\n",
    "#         outcome_copy.LABEL = np.random.choice(2,100,p=[0.9,0.1])\n",
    "#     outcome = pd.concat([outcome,outcome_copy],axis=0)\n",
    "\n",
    "# outcome.to_csv(\"./OUTCOME_COHORT.csv\", encoding='windows-1252')\n",
    "\n",
    "# person_copy= person.copy()\n",
    "\n",
    "# for i in range(10,20):\n",
    "#     person_copy2 = person.copy()\n",
    "#     person_copy2.PERSON_ID =int(i)\n",
    "#     person_copy = pd.concat([person_copy,person_copy2],axis=0)\n",
    "\n",
    "# person_copy.to_csv(\"./PERSON_NICU.csv\", encoding='windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "task_id = 'local_test'\n",
    "task_path = os.path.join(data_path, 'volume', task_id)\n",
    "log_path = os.path.join(data_path, 'volume', 'logs')\n",
    "task_log_path = os.path.join(log_path, task_id)\n",
    "if not os.path.exists(task_path):\n",
    "  os.mkdir(task_path)\n",
    "if not os.path.exists(log_path):\n",
    "  os.mkdir(log_path)\n",
    "if not os.path.exists(task_log_path):\n",
    "  os.mkdir(task_log_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data\\\\volume\\\\local_test'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data_loader, fraction):\n",
    "        'Initialization'\n",
    "        self.xt, self.yt,self.nx, self.ny = data_loader()\n",
    "        self.fraction = fraction\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.nx) / 5))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "            #positive_valid patient중 negative data undersampling    \n",
    "        rand_false2 = np.random.choice(self.nx.shape[0], size=int(np.floor(self.nx.shape[0]*self.fraction)))\n",
    "        random_nx = self.nx[rand_false2]\n",
    "        random_ny = self.ny[rand_false2]\n",
    "        \n",
    "        train_x = np.concatenate([self.xt,random_nx], axis=0)\n",
    "        train_y = np.concatenate([self.yt,random_ny], axis=0)\n",
    "            \n",
    "        if len(train_x) == len(train_y):\n",
    "            p = np.random.permutation(len(train_x))\n",
    "            train_x = train_x[p]\n",
    "            self.train_y = train_y[p]  \n",
    "        else:\n",
    "            print(\"there is non match\")\n",
    "        self.train_x = pad_sequences(train_x, padding='post', value=-5)\n",
    "        return (self.train_x, self.train_y)   \n",
    "    \n",
    "    def shape(self):\n",
    "        return self.train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.013001441955566406\n",
      "data_loader extract_person time: 0.010003805160522461\n",
      "data_loader extract_condition time: 0.007995128631591797\n",
      "data_loader extract_measurement time: 2.696000337600708\n",
      "data_loader groupby_hour_condition time: 0.010998010635375977\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 9.663999795913696\n",
      "data_loader make_person_sequence time: 0.1340022087097168\n",
      "X (1592,)\n",
      "Y (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.28100061416625977\n",
      "on_split\n",
      "data_loader split_data time: 0.04399514198303223\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                        is_train=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d = data_loader.get_train_data()\n",
    "x,y,z,t = data_loader.get_valid_data()\n",
    "\n",
    "# data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "#                          common_path=os.path.join(data_path, 'volume'),\n",
    "#                          task_path=task_path)\n",
    "train_datagen = DataGenerator(data_loader.get_train_data,fraction = 0.2)\n",
    "valid_datagen = DataGenerator(data_loader.get_valid_data,fraction = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fit_generator in module keras.engine.training:\n",
      "\n",
      "fit_generator(self, generator, steps_per_epoch=None, epochs=1, verbose=1, callbacks=None, validation_data=None, validation_steps=None, validation_freq=1, class_weight=None, max_queue_size=10, workers=1, use_multiprocessing=False, shuffle=True, initial_epoch=0)\n",
      "    Trains the model on data generated batch-by-batch by a Python generator\n",
      "    (or an instance of `Sequence`).\n",
      "    \n",
      "    The generator is run in parallel to the model, for efficiency.\n",
      "    For instance, this allows you to do real-time data augmentation\n",
      "    on images on CPU in parallel to training your model on GPU.\n",
      "    \n",
      "    The use of `keras.utils.Sequence` guarantees the ordering\n",
      "    and guarantees the single use of every input per epoch when\n",
      "    using `use_multiprocessing=True`.\n",
      "    \n",
      "    # Arguments\n",
      "        generator: A generator or an instance of `Sequence`\n",
      "            (`keras.utils.Sequence`) object in order to avoid\n",
      "            duplicate data when using multiprocessing.\n",
      "            The output of the generator must be either\n",
      "            - a tuple `(inputs, targets)`\n",
      "            - a tuple `(inputs, targets, sample_weights)`.\n",
      "            This tuple (a single output of the generator) makes a single\n",
      "            batch. Therefore, all arrays in this tuple must have the same\n",
      "            length (equal to the size of this batch). Different batches may\n",
      "            have different sizes. For example, the last batch of the epoch\n",
      "            is commonly smaller than the others, if the size of the dataset\n",
      "            is not divisible by the batch size.\n",
      "            The generator is expected to loop over its data\n",
      "            indefinitely. An epoch finishes when `steps_per_epoch`\n",
      "            batches have been seen by the model.\n",
      "        steps_per_epoch: Integer.\n",
      "            Total number of steps (batches of samples)\n",
      "            to yield from `generator` before declaring one epoch\n",
      "            finished and starting the next epoch. It should typically\n",
      "            be equal to `ceil(num_samples / batch_size)`\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(generator)` as a number of steps.\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire data provided,\n",
      "            as defined by `steps_per_epoch`.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: Integer. 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See [callbacks](/callbacks).\n",
      "        validation_data: This can be either\n",
      "            - a generator or a `Sequence` object for the validation data\n",
      "            - tuple `(x_val, y_val)`\n",
      "            - tuple `(x_val, y_val, val_sample_weights)`\n",
      "            on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data.\n",
      "        validation_steps: Only relevant if `validation_data`\n",
      "            is a generator. Total number of steps (batches of samples)\n",
      "            to yield from `validation_data` generator before stopping\n",
      "            at the end of every epoch. It should typically\n",
      "            be equal to the number of samples of your\n",
      "            validation dataset divided by the batch size.\n",
      "            Optional for `Sequence`: if unspecified, will use\n",
      "            the `len(validation_data)` as a number of steps.\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections.Container` instance (e.g. list, tuple, etc.). If an\n",
      "            integer, specifies how many training epochs to run before a new\n",
      "            validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only). This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples\n",
      "            from an under-represented class.\n",
      "        max_queue_size: Integer. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Maximum number of processes to spin up\n",
      "            when using process-based threading.\n",
      "            If unspecified, `workers` will default to 1. If 0, will\n",
      "            execute the generator on the main thread.\n",
      "        use_multiprocessing: Boolean.\n",
      "            If `True`, use process-based threading.\n",
      "            If unspecified, `use_multiprocessing` will default to `False`.\n",
      "            Note that because this implementation\n",
      "            relies on multiprocessing,\n",
      "            you should not pass non-picklable arguments to the generator\n",
      "            as they can't be passed easily to children processes.\n",
      "        shuffle: Boolean. Whether to shuffle the order of the batches at\n",
      "            the beginning of each epoch. Only used with instances\n",
      "            of `Sequence` (`keras.utils.Sequence`).\n",
      "            Has no effect when `steps_per_epoch` is not `None`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "    \n",
      "    # Returns\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    # Raises\n",
      "        ValueError: In case the generator yields data in an invalid format.\n",
      "    \n",
      "    # Example\n",
      "    \n",
      "    ```python\n",
      "    def generate_arrays_from_file(path):\n",
      "        while True:\n",
      "            with open(path) as f:\n",
      "                for line in f:\n",
      "                    # create numpy arrays of input data\n",
      "                    # and labels, from each line in the file\n",
      "                    x1, x2, y = process_line(line)\n",
      "                    yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n",
      "    \n",
      "    model.fit_generator(generate_arrays_from_file('/my_file.txt'),\n",
      "                        steps_per_epoch=10000, epochs=10)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Model.fit_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Input((None, 122))\n",
    "x = model_input\n",
    "\n",
    "x = Masking(mask_value=-5)(x)\n",
    "x = Dense(64, activation = 'tanh',name = '1stDense')(x)\n",
    "rnn_layers = [32]\n",
    "for idx, node in enumerate(rnn_layers):\n",
    "  return_sequences = False if idx == len(rnn_layers) - 1 else True\n",
    "\n",
    "  x = GRU(node,activation = 'relu',\n",
    "          return_sequences=return_sequences)(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "\n",
    "model_output = Dense(1, activation = 'sigmoid', kernel_initializer=TruncatedNormal(stddev=0.01))(x)\n",
    "\n",
    "loss = 'binary_crossentropy'\n",
    "\n",
    "model = Model(model_input, model_output)\n",
    "model.compile(optimizer=adam(lr =0.001), loss=loss, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69987, saving model to ./data\\volume\\local_test\\model-01-0.699868.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.69987 to 0.69887, saving model to ./data\\volume\\local_test\\model-02-0.698872.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69887 to 0.69839, saving model to ./data\\volume\\local_test\\model-03-0.698387.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69839 to 0.68854, saving model to ./data\\volume\\local_test\\model-04-0.688543.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.68854 to 0.67857, saving model to ./data\\volume\\local_test\\model-05-0.678566.hdf5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.67857\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.67857 to 0.66291, saving model to ./data\\volume\\local_test\\model-07-0.662913.hdf5\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.66291 to 0.65606, saving model to ./data\\volume\\local_test\\model-08-0.656060.hdf5\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.65606 to 0.63676, saving model to ./data\\volume\\local_test\\model-09-0.636762.hdf5\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.63676 to 0.60892, saving model to ./data\\volume\\local_test\\model-10-0.608923.hdf5\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.60892 to 0.60220, saving model to ./data\\volume\\local_test\\model-11-0.602202.hdf5\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.60220 to 0.58105, saving model to ./data\\volume\\local_test\\model-12-0.581049.hdf5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-f88f5ddd7d26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m model.train(train_datagen, valid_datagen, epochs=200, valid_steps = 10, \n\u001b[1;32m---> 19\u001b[1;33m             step_epoch = 10, verbose=0, callbacks=callbacks,workers=-1)\n\u001b[0m",
      "\u001b[1;32mD:\\hack\\prism3\\prism\\docker\\src\\model.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, traingen, valid_gen, epochs, valid_steps, step_epoch, verbose, callbacks, workers)\u001b[0m\n\u001b[0;32m     53\u001b[0m     self.model.fit_generator(generator= traingen, validation_data=valid_gen,\n\u001b[0;32m     54\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m                     verbose=verbose, callbacks=callbacks,workers=workers)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    240\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                             workers=0)\n\u001b[0m\u001b[0;32m    243\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1789\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1791\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1793\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, callbacks, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    399\u001b[0m             outs = model.test_on_batch(x, y,\n\u001b[0;32m    400\u001b[0m                                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m                                        reset_metrics=False)\n\u001b[0m\u001b[0;32m    402\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1557\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1559\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3292\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\.conda\\envs\\park\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=os.path.join(task_path, 'model-{epoch:02d}-{val_loss:2f}.hdf5'),\n",
    "                    monitor='val_loss',\n",
    "                    mode='min',\n",
    "                    save_best_only=True,\n",
    "                    save_weights_only=False,\n",
    "                    verbose=True\n",
    "    ),\n",
    "    TensorBoard(log_dir=task_log_path,\n",
    "                write_graph=True\n",
    "    ),\n",
    "     EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=2, mode='auto')\n",
    "\n",
    "]\n",
    "    \n",
    "model = SimpleRNNModel(shape=122)\n",
    "\n",
    "model.train(train_datagen, valid_datagen, epochs=200, valid_steps = 10, \n",
    "            step_epoch = 10, verbose=0, callbacks=callbacks,workers=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,a2 = train_datagen.__getitem__(1)\n",
    "\n",
    "train_datagen.__len__()\n",
    "\n",
    "a1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model prediction (inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_loader extract_outcome_cohort time: 0.0110015869140625\n",
      "data_loader extract_person time: 0.009998559951782227\n",
      "data_loader extract_condition time: 0.007002830505371094\n",
      "data_loader extract_measurement time: 2.248997688293457\n",
      "data_loader groupby_hour_condition time: 0.016000032424926758\n",
      "there is Na after interpolation\n",
      "data_loader groupby_hour_measurement time: 7.6969990730285645\n",
      "data_loader make_person_sequence time: 0.07199954986572266\n",
      "X (1592,)\n",
      "Key (1592, 2)\n",
      "data_loader make_data time: 0.19500136375427246\n",
      "not_in_split\n",
      "data_loader split_data time: 0.06700372695922852\n"
     ]
    }
   ],
   "source": [
    "from docker.src.data_loader import DataLoader\n",
    "data_loader = DataLoader(data_path=os.path.join(data_path, 'train'),\n",
    "                         common_path=os.path.join(data_path, 'volume'),\n",
    "                         task_path=task_path,\n",
    "                        is_train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docker.src.model import SimpleRNNModel\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = SimpleRNNModel(shape=122)\n",
    "\n",
    "# 모델 로드\n",
    "model2=model.load(task_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x = data_loader.get_infer_data()\n",
    "test_x.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_key = data_loader.key\n",
    "y_pred = model.predict(test_x)\n",
    "y_pred = pd.DataFrame(y_pred, columns=['pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.concat([y_key, y_pred], axis=1)\n",
    "\n",
    "out_key = data_loader.cohort_df[['SUBJECT_ID', 'COHORT_END_DATE']]\n",
    "merged_df = pd.merge(out_key, y_pred, on=['SUBJECT_ID', 'COHORT_END_DATE'], how='left')\n",
    "\n",
    "merged_df['LABEL_PROBABILITY'] = merged_df.pred\n",
    "merged_df['LABEL'] = (merged_df['LABEL_PROBABILITY'] > thr).astype(np.int32)\n",
    "\n",
    "merged_df[['LABEL', 'LABEL_PROBABILITY']].to_csv(os.path.join(data_path, 'output', 'output.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "park",
   "language": "python",
   "name": "park"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
